{"pages":[{"title":"AboutMe","text":"关于我（About me）一名普通的程序员、代码工人 曾就职于阿里、华为、腾讯 热爱编程、但往往被发现在搬砖 喜欢折腾，爱玩机、玩软件，偏爱知识管理 常常发现身边的小美好","link":"/about/index.html"},{"title":"All Categories","text":"","link":"/categories/index.html"},{"title":"pmp correcting book","text":"记录一些我做过的PMP错题。","link":"/pmp-correcting-book/index.html"},{"title":"All tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"A Glance Through the Window of Office (Two Pictures)","text":"两张旧照，从公司窗外，看​湿地和城市天际线。","link":"/2021/07/18/A-Glance-Through-the-Window-of-Office-Two-Pictures/"},{"title":"A Lovely Sunday Without Procrastination","text":"I was about to write something on Procrastination, but it’s such a huge topic for me to make it clear and solvable. What is it? Pro-Cra-Sti-Nation procrastination is avoiding doing someting. it’s not being able to get started. it’s trying to avoid the inevitable. it’s over complicating things yourself. it’s being afraid of finishing something. some not knowning when, some not knowning how to finish someting. How about this, I’ll share some things that are intentionally or unintentionally delayed by myself, at last they got finished and I could not be more happy about that. My toilet got broken about three weeks ago, I repaire it today and now it’s happily flushing. Kitchenware should have be ready a month ago, today they are ready ready and I cook my supper. A tiny problem on one of my working projects got fixed before dawn this morning. … All these are just exciting, and I think it’s good for me to keep on disassebling The Bombs. Reference: http://www.tudou.com/programs/view/LqRt_JYyK5o/","link":"/2016/10/20/A-Lovely-Sunday-Without-Procrastination/"},{"title":"A Simple Guess Number Game","text":"#include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;time.h&gt; #define NO 0 #define YES 1 int main(void) { int guess_value = -1; int number; int nbr_of_guesses; int done = NO; //获取随机数 srand((unsigned)time(NULL)); number = abs(rand()%100)+1;/*注意rand函数使用前要用到上面的srand那一行代码 产生一个1~100的随机数*/ nbr_of_guesses = 0; while(done == NO) { printf(&quot;/nPick a number between 0 and %d/n&quot;,100); scanf(&quot;%d&quot;,&amp;guess_value);//输入一个数 nbr_of_guesses ++;//猜测次数+1 if(number == guess_value) { done = YES; } else { if(number &lt; guess_value) { printf(&quot;/nLower,Please!&quot;); } if(number &gt; guess_value) { printf(&quot;/nHigher,Please!&quot;); } } } printf(&quot;/n/nCongratulations! You guessed right in %d Guesses!&quot; ,nbr_of_guesses); printf(&quot;/n/nThe number was %d/n/n&quot;,number); return 0; }","link":"/2010/11/17/A-Simple-Guess-Number-Game/"},{"title":"A Troubleshooting Case On Java 100 Percent CPU Usage","text":"早上遭遇一起”线上 Java Web 占用 100%CPU”的紧急事件, 将排查处理过程记录如下。 仅有的三台机器同时报警, 初步估计是个普遍性问题. 迅速重启其中两台应用, 并负责提供服务. 第三台机器用于保留现场, 进行后续排查. 登上机器, 执行下列操作, 确定出现问题的线程栈. * 确定Java进程号 &lt;pid&gt; ps -ef | grep &lt;app&gt; | grep -v grep * 确定耗费CPU资源的线程 &lt;pid_sub&gt; ps -Lfp &lt;pid&gt; ps -mp &lt;pid&gt; -o THREAD, tid, time top -Hp &lt;pid&gt; * 将线程号转变为十六进制(因为jstack中以十六进制确定线程ID) printf &quot;%x\\n&quot; &lt;pid_sub&gt; --- 例如输出 54ee * 在Java进程的jstack信息中确定 0x54ee 的位置 jstack &lt;pid&gt; | grep 54ee --- 例如输出 &quot;http-bio-7001-exec-4&quot; #59 daemon prio=5 os_prio=0 tid=0x00002aaac2e5c000 nid=0x966 runnable [0x000000004434e000]... --- 下面就是具体线程栈信息 通过问题线程栈确定业务逻辑位置, 最终定位到函数级别, 由此确定问题: 在一个特殊SQL文本的解析中, 存在indexOf返回-1, 并导致死循环的代码段, 细节不表. 教训: 代码测试一定要完善, 覆盖所有边界情况.","link":"/2017/08/10/A-Troubleshooting-Case-On-Java-100-Percent-CPU-Usage/"},{"title":"A Troubleshooting Case On Mixed Logging Frameworks In Java Maven Projects","text":"一次日志框架排坑记录一个Java项目中，由于引入的依赖可能使用各种不同的日志框架，并且各个模块的开发人员在没有硬性要求时会习惯性地使用自己熟悉的框架，这就导致项目中对各种日志框架的依赖各式各样，排查起来很麻烦。 例如，项目写日志的代码中，有直接使用Log4j的这种写法： // 使用 log4j.log4j 1.2.17 import org.apache.log4j.Logger; ... private Logger logger = Logger.getLogger(DBConnectionUtils.class); logger.error(&quot;error&quot;); ... 也有直接使用 Log4j 2的写法： // 使用 org.apache.logging.log4j.log4j-api 2.10.0 import org.apache.logging.log4j.LogManager; import org.apache.logging.log4j.Logger; ... private Logger logger = LogManager.getLogger(&quot;loggerName&quot;); logger.error(&quot;error&quot;); ... 还有使用 Slf4j 的写法： // 使用 org.slf4j.slf4j-api 1.7.25 import org.slf4j.Logger; import org.slf4j.LoggerFactory; ... public Logger logger = LoggerFactory.getLogger(&quot;loggerName&quot;); logger.error(&quot;error&quot;); ... // 或使用 lombok 的Slf4j注解 import lombok.extern.slf4j.Slf4j; // 利用注解产生log变量 @Slf4j public class TestClass { public void testMethod() { log.error(&quot;error&quot;); } } 由于项目日志的配置使用的log4j.xml，对应 log4j2 ，因此 log4j1 的使用应该是无效的，所有显式使用log4j1的地方需要全部改掉。 Slf4j 是一个日志接口，能兼容多种日志实现，如 log4j / JCL(java commons logging) / logback / java.util.logging , 能带来很好的扩展性，建议使用。因此所有显式使用 log4j2 的代码也需要改成 slf4j 。 由于三种写法的代码文件都很多，必须写一个脚本来实现代码替换，预先将目标文件的路径存放于 tmp_list 中，然后在同目录下执行下列脚本： #!/bin/sh # Env: MacBookPro11,4; 版本 10.13.3（版号 17D47） for DIR in `cat tmp_list`; do echo $DIR # 替换import org.apache.log4j.Logger;这行 find $DIR -type f -name *.java -exec sed -i '' '/import org.apache.log4j.Logger;/d' {} \\; # 替换*Logger logger =*这行 find $DIR -type f -name *.java -exec sed -i '' '/Logger logger =/d' {} \\; # 替换logger为log find $DIR -type f -name *.java -exec sed -i '' 's/logger/log/g' {} \\; # 在public abstract class上一行添加@Slf4j find $DIR -type f -name *.java -exec sed -i '' '/public abstract class/i\\ @lombok.extern.slf4j.Slf4j\\ ' {} \\; # 在public class上一行添加@Slf4j find $DIR -type f -name *.java -exec sed -i '' '/public class/i\\ @lombok.extern.slf4j.Slf4j\\ ' {} \\; done; 执行结束后，代码编译通过，说明没有明显的替换错误。但是工程启动后，发现有报错，TDDL 客户端抛出了以下异常(已经隐去不相干的若干行)： JM.Log:ERROR Failed to get Slf4jLogger java.lang.IllegalArgumentException: delegate must be logback impl or slf4j-log4j impl at com.taobao.middleware.logger.slf4j.Slf4jLogger.&lt;init&gt;(Slf4jLogger.java:50) at com.taobao.middleware.logger.slf4j.Slf4jLoggerFactory.getLogger(Slf4jLoggerFactory.java:17) at com.taobao.middleware.logger.LoggerFactory.getLogger(LoggerFactory.java:60) at com.taobao.middleware.logger.LoggerFactory.getLogger(LoggerFactory.java:69) at com.taobao.diamond.client.impl.LogUtils.logger(LogUtils.java:36) at com.taobao.diamond.client.impl.DiamondEnv.&lt;clinit&gt;(DiamondEnv.java:696) at com.taobao.diamond.client.impl.DiamondEnvRepo.&lt;clinit&gt;(DiamondEnvRepo.java:93) at com.taobao.tddl.config.diamond.DiamondConfigDataHandler.doInit(DiamondConfigDataHandler.java:50) at com.taobao.tddl.common.model.lifecycle.AbstractLifecycle.init(AbstractLifecycle.java:32) at com.taobao.tddl.config.impl.PreheatDataHandler.doInit(PreheatDataHandler.java:37) at com.taobao.tddl.common.model.lifecycle.AbstractLifecycle.init(AbstractLifecycle.java:32) at com.taobao.tddl.config.impl.UnitConfigDataHandlerFactory.getConfigDataHandler(UnitConfigDataHandlerFactory.java:66) at com.taobao.tddl.config.impl.UnitConfigDataHandlerFactory.getConfigDataHandler(UnitConfigDataHandlerFactory.java:52) at com.taobao.tddl.matrix.jdbc.TDataSource.loadConnectionProperties(TDataSource.java:197) at com.taobao.tddl.matrix.jdbc.TDataSource.doInit(TDataSource.java:127) at com.taobao.tddl.common.model.lifecycle.AbstractLifecycle.init(AbstractLifecycle.java:32) ... ... at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:829) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:357) at sun.rmi.transport.Transport$1.run(Transport.java:200) at sun.rmi.transport.Transport$1.run(Transport.java:197) at java.security.AccessController.doPrivileged(Native Method) at sun.rmi.transport.Transport.serviceCall(Transport.java:196) at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:568) at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:826) at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$0(TCPTransport.java:683) at java.security.AccessController.doPrivileged(Native Method) at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:682) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: java.lang.NullPointerException at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:264) at com.taobao.middleware.logger.slf4j.Slf4jLogger.&lt;init&gt;(Slf4jLogger.java:46) ... 115 more 通过Debug，发现问题出在这里： 从上图可以看到，tddl 底层的中间件代码中，日志代理类只可以是 ch.qos.logback.classic.Logger 和 org.slf4j.impl.Log4jLoggerAdapter, 而我们的代理类却默认初始化成了 org.apache.logging.slf4j.Log4jLogger, 为什么会出现这个情况呢？猜测 log4j-slf4j-impl 在初始化的时候是优先于 slf4j-log4j12 的 ，把 org.apache.logging.slf4j.Log4jLogger 所在的如下依赖从 pom 中去掉看看。 结果初始化时恢复到了 org.slf4j.impl.Log4jLoggerAdapter 了，问题得到解决。此时，比较下 log4j-slf4j-impl 和 slf4j-log4j12 的区别，我们可以得出以下结论： log4j-slf4j-impl 是 slf4j 和 logj4 2 的 Binding，而 slf4j-log4j12 是 slf4j 和 log4j 1.2 的 Binding，com.taobao.middleware 这项服务底层硬编码只支持 slf4j-log4j12 和 logback ，并不支持 log4j2，尽管 log4j2 已经用于很多大型项目了。","link":"/2018/03/02/A-Troubleshooting-Case-On-Mixed-Logging-Frameworks-In-Java-Maven-Projects/"},{"title":"Adaptance","text":"About a month ago, I graduated with a M.S. degree from Chengdu, and now end in Alibaba which is one of the biggest internet company in China. Graduations are both painful and joyful, but for me they are more like thrilling. The eager to explore something new and the just proper fearness can excite everyone. Hanzhou is a beautiful tourist city as I heard about, and worth exploring whether it’s a good place to live. About 33% larger than Chengdu in area, it holds only 62% of Chengdu’s population. The downtown area, 314 square kilometers, is a shrinking pit relative to Chengdu’s 598 square kilometers. Be this as it may, the area of the city really has little to do with me, :) A profession like software engineers, should be pragmatic in a lot of ways. Programmers are always serious, precise, rigorous to people. Can they be romantic? Yes. I may learn some skills for life now, for the real pragmatism purpose.","link":"/2015/07/25/Adaptance/"},{"title":"Alfred New User Notes - Part I","text":"大概是5年前，开始使用Mac电脑作为主力工具的。5年间由于换工作的原因，工作电脑从Mac换到Windows，又换回Mac。但个人电脑则一直都是Mac。 年初换了新工作，工作内容全部用Mac来承载了，想起来这么多年竟然没碰过Mac上的“神器”软件Alfred，怎么着都得来试试，好好提高下效率和幸福感。 既然要把工作负载都交给它了，必须要相信它，例如放开访问通讯录和1Password的权限等等。说实在的，放开1Password权限起初还是有些不太敢，毕竟这软件有个后门什么的，岂不是全部家当给丢出去了。但转念，还是这么做了，原因有三：Alfred是成名多年的优秀软件，如果有后门肯定早曝光了；坚决使用正版软件，免受盗版软件荼毒，毕竟全部家当都交了，不能冒这个险；实在不行，采用最后手段，禁Alfred的网络权限。 下面就说下我（作为一名新用户）的购买、配置和使用过程吧。 首先，Alfred只能通过官网下载，无法使用AppStore安装。原因在官网上有讲到，https://www.alfredapp.com/help/troubleshooting/mac-app-store/ 。所以先行下载吧。https://www.alfredapp.com/ 购买流程也相当简单，进入购买页面https://www.alfredapp.com/shop/ ，选择套餐版本（有29英镑和49英镑两种License，区别是前者仅包含当前版本4，后者包含终生升级），我选了终生升级版。接着输入名字、邮箱，点击购买。进入付款页面后，有两种付费方式，信用卡和Paypal，我用的是信用卡（Visa）支付。付款后，几分钟内，收到了收据和License邮件。打开软件，点击左侧边栏中的PowerPack，输入邮箱和上述收到License，便激活了PowerPack。对了，使用Alfred，一定要购买PowerPack，不然。。。就等于不用了。 接下来，介绍下我平时使用最多的两个功能，Clipboard History和System Monitor Workflow。 先说Clipboard History，功能设置入口位于左侧边栏Feature =&gt; Clipboard History。功能使用上，顾名思义，便是能够通过它访问剪贴板历史。对于我们这种工程人员，经常复制（⌘+C）的场景，会用到历史上复制过的内容，这个工具能够很好地弥补。遍历剪贴板历史和使用旧的复制内容不再麻烦，体验很好。它还有个颇具特色的能力，便是自动将连续复制（⌘+C）的内容合并，例如用户希望复制一段话中的两个关键词，他能够通过迅速复制关键词1和关键词2，而剪贴板的内容将会是关键词1+关键词2。通过设置，还可以在合并时自动插入换行符，甚至，还能够在合并时发出声音。发出声音这个功能个人使用后觉得无法适应，偶发的Pyur-pyur声经常被我当作是什么地方的异响，相当地interrupting。 然后是System Monitor工作流。这是个第三方工作流，主要功能可以看这里 https://github.com/singhprd/system-monitor-alfred-workflow/blob/master/README.md ，概括来讲，便是能够通过Alfred的快捷命令查看系统性能，如内存、CPU、IO使用率等。安装依然简单，通过刚刚的链接进去可下载到这个工作流的.workflow文件，点击打开便能直接被导入Alfred了。 接下来的事情，就比较Tricky了。它并不是安装即可用的，由于它只固定地使用某个旧版本的ruby程序，而在新的Mac系统中ruby是不断更新的，由此造成了某些目录不匹配的问题。如图所示： 经过一番搜索，在这里 https://github.com/zhaocai/alfred2-sourcetree-workflow/issues/7#issuecomment-633050353 找到了答案。报错信息中提到的2.6.0目录如果不存在，那么我们就通过软链接建一个目录出来。之后便畅通无阻了。 总结：这是我目前使用Alfred一周以来，用得最畅快的两个功能了。难怪说它是“神器”，后续若发现更多技巧，我也会及时拿出来跟大家分享。","link":"/2021/07/17/Alfred-New-User-Notes-Part-I/"},{"title":"An Introduction To iptables","text":"概述Iptables是Linux内核防火墙(netfilter)提供的通用的表结构，和netfilter是早期Linux版本中ipchains和ipfwadm的继任者。iptables用于网络地址转换（NAT）、封包过滤（packet filtering），并在Linux 2.4版本后提供封包修改（packet mangling）。 这里放上IP封包头部的结构，以便于下文对各个规则的理解。 图1-1 IP包结构 相关字段说明： Version：IP协议的版本（如IPv4） IHL：IP Header的长度 DSCP：服务代码（即Type of Service） ECN：显式拥塞指示标记 Total Length：整个IP包的长度 Identification：身份标识。如果一个IP包在传输过程中分段了，同一个包的不同片段拥有相同的唯一标识 Flags：标识是否能分段传输 Fragment Offset：分段偏移量。标识本分段在原始IP包中的详细位置 Time to Live：为避免网络回路（loop），每个Packet发送时都带有一个TTL值，表示这个Packet可以经过多少次路由（即“多少跳”）。每经过一跳，该值减1，达到0该Packet被丢弃。 Protocol：告诉目标host的网络层，自己属于哪个协议。（ICMP-1，TCP-6，UDP-17） Header Checksum：头部校验，用于检查Packet是否有错误 Source Address：数据包的32位发送地址 Destination Address：数据包的32位接收地址 Options：可选字段，仅在IHL大于5时使用。可以包含Security、Record Route、Timestamp等信息。 规则表(Tables)Iptables内建规则表有三个：filter、nat、mangle。 Mangle表Mangle是英文“破坏、损坏”的意思，Mangle表的功能是修改数据包头部，如修改TOS之类的值。（建议不要在这个表中做任何filter或nat相关的工作） Mangle表的几个target是：TOS、TTL、MARK、SECMARK、CONNSECMARK。其中TOS、TTL分别用于修改packet头部中的TOS、TTL字段；MARK、SECMARK、CONNSECMARK都用于向packet中设置可选的特殊字段。 Nat表用于不同数据包的NAT（Network Address Translation），即用于修改数据包的source和destination两个值。一段流中只有第一个数据包会被Nat表中的规则修改，剩余数据包自动与第一个包采取相同行为。包含的target是DNAT、SNAT、MASQUERADE、REDIRECT。 DNAT用于修改数据包的destination地址然后转发至新的host，例如将数据包从公网IP发送至内网IP。 SNAT用于修改数据包的source地址，例如从内网向公网发起连接时，防火墙可将内网地址替换为自身公网地址。 MASQUERADE与SNAT的用法基本相同，但是通过它处理的数据包的source地址是动态计算出来的，而非单个配置好的IP，所以MASQUERADE可看做SNAT的一种特殊形式。很多网络服务商提供的是动态IP（拨号上网），使用MASQUERADE可以达到SNAT无法达到的效果。 REDIRECT是DNAT的一种特殊形式，不管数据包头部的destination是哪里，都转发到本地host上。 Filter表Filter表是默认表，用于过滤Packet，可以依据Packet 内容选择DROP/ACCEPT等。 Security表Security表用于MAC（Mandatory Access Control，如SELinux等）网络规则控制。 规则链(Chains)链是多个有序的Rule序列的列表（即包含多个Rule列表的列表）。 表是由多个链组成的，默认表filter的内置链有INPUT、OUTPUT、FORWARD，这些链在Packet过滤过程的不同位置激活并生效（如图3-1所示）。 图3-1 iptables packet flow Nat表内置链是PREROUTING、OUTPUT、POSTROUTING Mangle表内置链是PREROUTING、OUTPUT，2.4.18后加入了INPUT 、FORWARD、POSTROUTING Raw表内置链是PREROUTING、OUTPUT 绝大多数情况下不会用到raw、mangle和security表，所以图3-1可以简化成图3-2。 图3-2 简化的iptables packet flow 规则(Rules)一条规则包含多个匹配条件和一个target（动作、处理行为）。 通常的匹配条件有接收数据包的网络接口（如eth0, eth1），数据包类型（如ICMP、TCP、UDP），数据包目的端口等。 Target可以是用户自定义链、内置链或扩展target。内置target有ACCEPT、DROP、QUEUE、RETURN；扩展target有REJECT、LOG。如果是内置target，Packet会被立即处理，不再Table中停留（即使还有后续链）。如果target是用户自定义链，Packet依然要走完原链的所有规则，才能跳到target（即这个自定义链）。 常用规则删除现有规则iptables –F 或 iptables –flush 设置默认链策略(将INPUT/FORWARD/OUTPUT的默认链策略设为DROP，这样做会屏蔽所有输入、输出网卡的数据包，除非你明确指定哪些数据包可以通过网卡。) iptables -P INPUT DROP iptables -P FORWARD DROP iptables -P OUTPUT DROP 设置黑名单1)屏蔽所指定的IP地址访问本地主机 iptables -A INPUT -i eth0 -s 192.168.43.0/24 -j DROP 2)仅屏蔽来自该IP的TCP数据包 iptables -A INPUT -i eth0 -p tcp -s 192.168.43.0/24 -j DROP 允许来自外部的ping并回应iptables -A INPUT -p icmp –icmp-type echo-request -j ACCEPT iptables -A OUTPUT -p icmp –icmp-type echo-reply -j ACCEPT 允许从本机ping外部主机iptables -A OUTPUT -p icmp –icmp-type echo-request -j ACCEPT iptables -A INPUT -p icmp –icmp-type echo-reply -j ACCEPT 允许回环(loopback)访问iptables -A INPUT -i lo -j ACCEPT iptables -A OUTPUT -o lo -j ACCEPT 允许进入eth0的ssh连接iptables -A INPUT -i eth0 -p tcp –dport 22 -m state –state NEW,ESTABLISHED -j ACCEPT iptables -A OUTPUT -o eth0 -p tcp –sport 22 -m state –state ESTABLISHED -j ACCEPT 允许从本地发起经过eth0的ssh连接iptables -A OUTPUT -o eth0 -p tcp –dport 22 -m state –state NEW,ESTABLISHED -j ACCEPT iptables -A INPUT -i eth0 -p tcp –sport 22 -m state –state ESTABLISHED -j ACCEPT 允许来自192.168.100.0/24网络的ssh连接(CIDR网络地址)iptables -A INPUT -i eth0 -p tcp -s 192.168.100.0/24 –dport 22 -m state –state NEW,ESTABLISHED -j ACCEPT iptables -A OUTPUT -o eth0 -p tcp –sport 22 -m state –state ESTABLISHED -j ACCEPT 仅允许从本地主机连接到192.168.100.0/24网络的ssh请求iptables -A OUTPUT -o eth0 -p tcp -d 192.168.100.0/24 –dport 22 -m state –state NEW,ESTABLISHED -j ACCEPT iptables -A INPUT -i eth0 -p tcp –sport 22 -m state –state ESTABLISHED -j ACCEPT 允许HTTP/HTTPS连接请求1)允许HTTP连接：80端口 iptables -A INPUT -i eth0 -p tcp –dport 80 -m state –state NEW,ESTABLISHED -j ACCEPT iptables -A OUTPUT -o eth0 -p tcp –sport 80 -m state –state ESTABLISHED -j ACCEPT 2)允许HTTPS连接：443端口 iptables -A INPUT -i eth0 -p tcp –dport 443 -m state –state NEW,ESTABLISHED -j ACCEPT iptables -A OUTPUT -o eth0 -p tcp –sport 443 -m state –state ESTABLISHED -j ACCEPT 允许从本地发起HTTPS连接本规则可以允许用户从本地主机发起HTTPS连接，从而访问Internet。 iptables -A OUTPUT -o eth0 -p tcp –dport 443 -m state –state NEW,ESTABLISHED -j ACCEPT iptables -A INPUT -i eth0 -p tcp –sport 443 -m state –state ESTABLISHED -j ACCEPT 使用-m multiport指定多个端口通过指定-m multiport选项，可以在一条规则中同时允许SSH、HTTP、HTTPS连接： iptables -A INPUT -i eth0 -p tcp -m multiport –dports 22,80,443 -m state –state NEW,ESTABLISHED -j ACCEPT iptables -A OUTPUT -o eth0 -p tcp -m multiport –sports 22,80,443 -m state –state ESTABLISHED -j ACCEPT 允许出站DNS连接iptables -A OUTPUT -p udp -o eth0 –dport 53 -j ACCEPT iptables -A INPUT -p udp -i eth0 –sport 53 -j ACCEPT 允许来自指定网络的rsync连接请求只希望能够从内部网络（192.168.101.0/24）访问即可： iptables -A INPUT -i eth0 -p tcp -s 192.168.101.0/24 –dport 873 -m state –state NEW,ESTABLISHED -j ACCEPT iptables -A OUTPUT -o eth0 -p tcp –sport 873 -m state –state ESTABLISHED -j ACCEPT 允许来自指定网络的MySQL连接请求你可能启用了MySQL服务，但只希望DBA与相关开发人员能够从内部网络（192.168.100.0/24）直接登录数据库： iptables -A INPUT -i eth0 -p tcp -s 192.168.100.0/24 –dport 3306 -m state –state NEW,ESTABLISHED -j ACCEPT iptables -A OUTPUT -o eth0 -p tcp –sport 3306 -m state –state ESTABLISHED -j ACCEPT 允许Sendmail, Postfix邮件服务(端口25)iptables -A INPUT -i eth0 -p tcp –dport 25 -m state –state NEW,ESTABLISHED -j ACCEPT iptables -A OUTPUT -o eth0 -p tcp –sport 25 -m state –state ESTABLISHED -j ACCEPT 允许IMAP与IMAPSIMAP：143iptables -A INPUT -i eth0 -p tcp –dport 143 -m state –state NEW,ESTABLISHED -j ACCEPT iptables -A OUTPUT -o eth0 -p tcp –sport 143 -m state –state ESTABLISHED -j ACCEPT IMAPS：993iptables -A INPUT -i eth0 -p tcp –dport 993 -m state –state NEW,ESTABLISHED -j ACCEPT iptables -A OUTPUT -o eth0 -p tcp –sport 993 -m state –state ESTABLISHED -j ACCEPT 允许POP3与POP3SPOP3：110iptables -A INPUT -i eth0 -p tcp –dport 110 -m state –state NEW,ESTABLISHED -j ACCEPT iptables -A OUTPUT -o eth0 -p tcp –sport 110 -m state –state ESTABLISHED -j ACCEPT POP3S：995iptables -A INPUT -i eth0 -p tcp –dport 995 -m state –state NEW,ESTABLISHED -j ACCEPT iptables -A OUTPUT -o eth0 -p tcp –sport 995 -m state –state ESTABLISHED -j ACCEPT 防DoS攻击iptables -A INPUT -p tcp –dport 80 -m limit –limit 25/minute –limit-burst 100 -j ACCEPT -m limit: 启用limit扩展 –limit 25/minute: 允许最多每分钟25个连接 –limit-burst 100: 当达到100个连接后，才启用上述25/minute限制 允许网卡间路由如果本地主机有两块网卡，一块连接内网(eth0)，一块连接外网(eth1)，那么可以使用下面的规则将eth0的数据路由到eht1： iptables -A FORWARD -i eth0 -o eth1 -j ACCEPT DNAT与端口转发以下规则将会把来自422端口的流量转发到22端口。这意味着来自422端口的SSH连接请求与来自22端口的请求等效。 1.启用DNAT转发iptables -t nat -A PREROUTING -p tcp -d 192.168.102.37 –dport 422 -j DNAT –to-destination 192.168.102.37:22 2.允许连接到422端口的请求iptables -A INPUT -i eth0 -p tcp –dport 422 -m state –state NEW,ESTABLISHED -j ACCEPT iptables -A OUTPUT -o eth0 -p tcp –sport 422 -m state –state ESTABLISHED -j ACCEPT 将HTTP请求转发到内部服务器xxx.xxx.xxx.xxxiptables -t nat -A PREROUTING -p tcp -i eth0 -d xxx.xxx.xxx.xxx –dport 8888 -j DNAT –to 192.168.0.2:80 iptables -A FORWARD -p tcp -i eth0 -d 192.168.0.2 –dport 80 -j ACCEPT 当该数据包到达xxx.xxx.xxx.xxx后，需要将该数据包转发给192.168.0.2的80端口，事实上NAT所做的是修改该数据包的目的地址和目的端口号。然后再将该数据包路由给对应的主机。 但是iptables会接受这样的需要路由的包么？这就由FORWARD链决定。我们通过第二条命令告诉iptables可以转发目的地址为192.168.0.2:80的数据包。再看一下上例中422端口转22端口，这是同一IP，因此不需要设置FORWARD链。 SNAT与MASQUERADE如下命令表示把所有10.8.0.0网段的数据包SNAT成192.168.5.3的ip然后发出去： iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -o eth0 -j snat –to-source 192.168.5.3 对于snat，不管是几个地址，必须明确的指定要snat的IP。假如我们的计算机使用ADSL拨号方式上网，那么外网IP是动态的，这时候我们可以考虑使用MASQUERADE iptables -t nat -A POSTROUTING -s 10.8.0.0/255.255.255.0 -o eth0 -j MASQUERADE 负载均衡可以利用iptables的-m nth扩展，及其参数（–counter 0 –every 3 –packet x），进行DNAT路由设置（-A PREROUTING -j DNAT –to-destination），从而将负载平均分配给3台服务器： iptables -A PREROUTING -i eth0 -p tcp –dport 443 -m state –state NEW -m nth –counter 0 –every 3 –packet 0 -j DNAT –to-destination 192.168.1.101:443 iptables -A PREROUTING -i eth0 -p tcp –dport 443 -m state –state NEW -m nth –counter 0 –every 3 –packet 1 -j DNAT –to-destination 192.168.1.102:443 iptables -A PREROUTING -i eth0 -p tcp –dport 443 -m state –state NEW -m nth –counter 0 –every 3 –packet 2 -j DNAT –to-destination 192.168.1.103:443 自定义链，记录丢弃的数据包1.新建名为LOGGING的链iptables -N LOGGING 2.将所有来自INPUT链中的数据包跳转到LOGGING链中iptables -A INPUT -j LOGGING 3.指定自定义的日志前缀”IPTables Packet Dropped: “iptables -A LOGGING -m limit –limit 2/min -j LOG –log-prefix “IPTables Packet Dropped: “ –log-level 7 4.丢弃这些数据包iptables -A LOGGING -j DROP","link":"/2015/11/28/An-Introduction-To-iptables/"},{"title":"BORDER: Role Analysis","text":"Border是朝日电视台于2014年4月10日首播的惊悚悬疑电视剧，该剧讲述了内心倍感空虚和孤独的青年刑警石川安吾一次在例行巡查犯罪现场附近时，被神秘之人开枪击中头部而命悬一线。幸运的是石川最终从死亡线上活转过来，并意外拥有了可以跟死者进行对话的能力，并从此利用这种能力为追寻真相而四处奔走。 关于本剧，我想按分集剧情谈谈剧中人物在剧情中性格安排的合理性，以及带给观剧者的思考。 下面按序对每一集开始解析，解析中我会按照剧情简介、人物性格剖析及思考的顺序进行。 第一集 发现剧情简介：刑警石川安吾在案件调查中头部中弹，但奇迹般生还，残留于头部的子弹让他具有了与死者交谈的能力。回归警队后，遇到了一家三口被杀案，通过与死者的交谈，他获得了调查的线索，锁定了嫌疑人——受害者的邻居仓桥贵志。起初嫌疑人请母亲包庇、做不在场证明，后来母亲在监控录像面前无可辩驳，证明是仓桥贵志所为。 性格剖析：据其母亲的描述，嫌疑人仓桥贵志从小在母亲溺爱中长大，为儿子的为所欲为买单。仓桥贵志在母亲供述自己后，还试图狡辩，没有一点认罪担当的态度。剧中也暴露出仓桥贵志有贪婪、好色的性格偏向，可能是引发起杀人的动机之一。 “贪婪懦弱无担当，谁来庇护都枉然” 第二集 救出剧情简介：六名少女遇害，被锁定的嫌犯村上一夫在石川正欲调查之际毁灭证据并自杀。村上死后与石川交谈，将线索一点点给出，并不断发起挑衅，如果二十四小时内不破案便会出现下一名受害者。了解到村上是一个自大狂妄之辈后，石川以破坏其名声作为威胁，引诱其说出最后一名受害者被藏匿的地点，但无果。借助黑客以及村上前妻的协助，藏匿地点被找出，受害女子救出。 性格剖析：村上一夫在被前妻认为“不可长期相处”（即“不靠谱”）后，自尊收到巨大打击。而后开始不断引诱女子并强迫对方接受自己“生孩子”的要求，认为自己“伟大的基因”就应该传承下去，在被女子拒绝后遍便下杀手。 “狂妄无度不收敛，自食恶果是夜郎” 第三集 连锁剧情简介：受害人西本勇治惨死于町中广场，自治会长藤崎进入石川的视野。经调查，西本勇治正是五年前少年吸毒杀人案的凶手，五年来其隐姓埋名并整容以逃过公众视野，但由于想出书而请求当年凶案受害人家属原谅导致身份暴露，并引起受害人家属伤痛回忆。自治会长藤崎是五年前凶案受害人之一的叔叔，它忍无可忍，在町内数人帮助下对西本勇治下了杀手。 性格剖析：西本勇治未成年吸毒犯错，杀害别人妻儿，事件在网络上发酵，其早已成过街老鼠。藤崎则抱着“杀该杀之人无过”的信念，对仇人发泄怒火毫不迟疑。 “伤心地不可久留，求心安才是王道” 第四集 爆破剧情简介：市内多出宽阔场地不断出现带有爆炸陷阱的流浪汉尸体，验尸官比嘉美香在现场被炸弹波及受伤，显然这是一起针对刑事警察的连环案件。刑警石川在与死者对话后得知嫌疑人右手有灼伤疤痕的特征，与此同时，嚣张的嫌犯不断通过匿名电话对石川挑衅，并威胁伤害更多的警察。第三起案件发生后，美香提醒石川可疑人士是案发现场的年轻巡警，石川根据此线索在炸弹爆炸前瞬间阻止并抓获嫌犯。 性格剖析：嫌犯（无名）满足众多孤独的年轻人的特征，籍籍无名又想为人所知，制作炸弹试图引发轰动。在当警察的希望破灭后，产生报复心理，开始了以流浪汉尸体为诱饵制造警察伤亡的计划。 “恨比爱的力量强大，但它往往是不可持续的” 第五集 追忆剧情简介：死者刚部亦刚被发现死于庭院中，与石川交流时发现已完全失忆。警方初步确定为抢劫杀人案，通过找到死者妻子认领尸体，死者记忆恢复，讲述了事发当晚的来龙去脉和述生前与妻子的种种感人瞬间。原来，他只是认为工作前途已被限定，升职无望，内心苦恼又无处诉说，只好以出差为借口翘班出外潇洒。然而发生意外，心脏受到重创，与事发当晚凌晨死去。 性格剖析：当事人性格内向、木讷，工作调职苦闷无处诉说。内心迷茫之际，希望外出散心。但人在迷茫时，做出的事情往往不合情理。他像个小孩子一样把路边铁墩作为跑道上的跨栏，重心不稳导致胸部、脑部受到重创。回忆生活时，妻儿仍是他最大动力和生活下去的勇气之源。 “受困请开口求助，迷茫勿逃避处之” 第六集 苦恼剧情简介：两女子坠楼身亡，初步认定为自杀。但验尸官美香在现场发现有异样，此后发现两受害者均有自杀未遂的历史，进而通过黑客找到了两者公共的心理医生。在犯人欲再次出手时阻止，随后犯人跳楼自杀身亡。死后与石川对话，述说了年幼时姐姐在眼前跳楼自杀的经历，让其认为自杀之人无法拯救，因此对自杀者无需怜悯。 性格剖析：犯人性格偏激，受幼年阴影影响，对自杀行为由抵触上升为痛恨。恰恰自己又是心理医生，接触到的刺激不断增强，终于心理防御被攻破，“白衣天使”堕落为连环杀人犯。 “修神未果反成魔，都是执念惹的祸” 第七集 败北剧情简介：横森死于一起肇事逃逸案件，犯罪线索渐渐指向有背景的嫌疑人——外务大臣之子宇田川。证人受到威胁，证词不断被修改，奇怪男子不断消灭证据。最后，石川与奇怪男子正面冲突，却被告知宇田川已前往加拿大，此次案件也无法抓获罪魁祸首了。 性格剖析：本集剖析主角石川，在来自上司、嫌疑人、证人、线人的各种不利指责时，坚持查明真相，需要勇气。但这种执念没有给他带来胜利，面临如此强大阻力，坚毅如他也无法力挽狂澜。可正是奋力争取了，才迎来对手的尊重。 “真相之路，荆棘如血” 第八集 决断剧情简介：刑警荒木秀夫被灭口，手法与让石川受伤的枪击案十分相似。荒木前妻的口供牵扯出十年前的贩毒案，而负责当年贩毒案的正是石川的组长立花和顶头上司鸭川。石川用脑袋里的子弹线索为诱饵，引诱立花和鸭川跟踪，并让铃木在暗处拍摄，面对石川持枪质问，鸭川道出真相。 性格剖析：警内黑幕，牵涉出来的是挥霍、威胁、劫持、灭口。鸭川作为顶头上司，说出了“你不会永远走在正义的台阶上”这种话，认为石川只是热血的小年轻。但实际上，“原则”，总是有人与生俱来。 “原则和底线，有人注定守护不了，另一些人却与生俱来” 第九集 越境剧情简介：少年川弘志被诱拐杀害，在于死者交流中发现安藤就是真凶，但苦于没有证据。安藤又自诩为邪恶的贯彻者，与石川所执行的“模棱两可”的正义是天壤之别。愤怒又无可奈何的石川将安藤推下天台，在震惊和失望中，听到死后的安藤道出一句“欢迎来到这边的世界”。 性格剖析：本集可谓全剧败笔，反派安藤人物形象单薄，处于信息不透明的无懈可击态，石川在极度愤怒和无奈中推下石川，恐怕自己的人生也将完全改变。话说回来，本集中石川完全不像前面所有剧情中的“正义贯彻者”，在“绝对邪恶”面前也只能用最原始的手法——杀人——做最后一搏，但这样“正义”就堕落为“邪恶”，可能这才是本集要告诉观众的核心主题？ “究极的正义与邪恶，其实是一体” – 完 –","link":"/2017/01/07/BORDER-Role-Analysis/"},{"title":"Blackout And Programmers, From Despair To Concentration","text":"停电，停电，停电。 在读书时代，尤其是家乡还处于电力改造的小学时代，停电很常见。那时候，一停电，最担心的却是看不到每天傍晚的动画片，在烛光下写完作业就睡觉，倒也省下了和老哥争抢遥控器引发的争端。中学的时光里，停电带来的往往是混乱的狂欢，压抑下不多的闲暇机会。大学后，停电已经见怪不怪了，大家开始聚在一起想办法渡过这段被慢放的光阴。现在，由于工作原因几乎任何时候都离不开电脑，更不要提没有电的日子。 上个周五晚上，停电持续了一个小时。本来正在电脑前编码的我，突然不知所措，仿佛这世界离我远去了。也许只有经历过儿时看不到动画片的绝望，才能体会当时的心情。说不太清，好像此时的停电带给我的不是绝望，而是反思。 停电，让人思考。 时代让社会的生产力大幅提升，每个人拥有的生产资料和价值都远超以前，工作和生活充斥着数不清的设备、服务、活动、诱惑、干扰。这些“元素”与人的关联渠道便是人们可接触的媒介：电脑、手机等电子设备。一旦我们的生活完全寄托于电子设备，形成强依赖，那么就很难轻易摆脱它们，这也是2000年以来的潮流所向，今后又可能演变成如今大热的VR概念，或者很多科幻作品中出现过的全息广告视频。总之，信息的窗口越来越丰富，人们已经完全浸染其中，该不该抽身而出，如果应该，会是什么时候。 停电的发生，证明我们需要更专注。 我认为停电是好事，甚至是互联网中兴时代唯一一件因为其存在而值得欢喜雀跃的事情。当泛滥的信息之潮离我远去，我才能更关注一些更小但是息息相关的身边事，而非虚无缥缈的所谓明星、大V和种种虚的技术概念。 浮云散去，便是可怕又可爱的现实，别再沉迷于虚妄的技术之中了，实体世界才让我感觉到真实。","link":"/2016/12/30/Blackout-And-Programmers-From-Despair-To-Concentration/"},{"title":"Blockchain Notes: Origin","text":"区块链是人类记账历史走到现在，科技给我们的最新选择，是账本演变史上最新的一个高可行性的形态。 记账需求遥远的旧石器时代，记账全靠死记硬背和心算。随着生产力越来越高，单靠脑袋计数已经满足不了，于是，记录成了必须要改善的事情，产生了简单刻画和直观绘图的方法。 后来，人们发现绘图和刻画这些费力又占地方的记录方式又跟不上需求了，于是开始了结绳记事。结绳记事对记录对象、数量变化、最终结果都形成了确定的表现形式，几乎可称作账本的起源。 接下来，到了流传相对广泛的复式记账法。中国的复式记账法起源于明末清初的龙门帐，之后又发展成四脚账。而西方的则最早出现在12-13世纪。然后是19世纪，会计的诞生体现了信息技术爆炸后人们对于账本安全的担忧。而且在21世纪的今天，尽管信息化、数据化、智能化相当发达，仍然存在信息不对成及信用问题。 如何才能保证一个可公开的账户是绝对安全可靠的呢？ 信任危机互联网善于处理信息分享，却不能解决价值转移。 如今的中心化机构通过政府或集团公司的背书，把所有价值转移的计算都放在一个中心服务器上进行处理，其中一定会涉及人的参与，而人的“有限理论”和“机会主义行为”往往会使整个系统变得不那么可信。 区块链如何拯救信任危机：区块链是比特币金融系统中的核心技术，它的实质是一个不断增长的分布式结算数据库，能完美解决信息系统中的信任危机。 区块链技术可以很好地满足公信力需求，并把公信力抽象出来作为一个独立的而不是由政府或第三方组织掌控的存在，形成政府、大众、区块链与公信力互相监督的“公信新格局”。信任是建立在区块链上的，而非由单个组织掌控， 从而公信力可以被多方交叉验证与监督。 区块链的公信力区块链是分布式的，在网络上有许多独立节点，每个节点都有一份备份信息，每个有授权的人都可以从任意一个节点下载全部的信息，同时，区块链公信力网络也是不可篡改的，任何节点企图更改信息都会被其他节点发现，而更改的节点不会被确认，就会立刻丧失公信力。 在区块链公信力模型中，区块链不制定政策，它只是一个公证人角色，是政府建立和执行政策的工具。区块链的作用是帮助政府更快速和准确地让政策被全民接受与认可，同时，因为区块链是一个不变的、可以被复制的数据库，政府的政策就变得公开和透明。 比特币起源中本聪 - 《比特币：一种点对点的电子现金系统》 2008 讲述了比特币的几个基本原则： 一个纯粹的点对点电子现金系统，使在线支付能够直接由一方发起并支付给另一人，中间不需要通过任何金融机构。 不需要授信的第三方支持就能防止双重支付，点对点的网络环境是解决双重支付的一种方案。 对全部交易加上时间戳，并将他们并入一个不断延展的基于哈希算法的工作量证明的链条作为交易记录。除非重新完成全部的工作量证明，形成的交易记录将不可更改。 最长的链条不仅将作为被观察的事件序列的证明，而且将被视为来自CPU（中央处理器）的计算能力最大的池。只要大多数CPU的计算能力不被合作攻击的节点所控制，那么就会生成最长的、长度超过攻击者的链条。 这个系统本身需要的基础设施非常少，节点尽最大努力在全网传播信息即可，节点可以随时离开和重新加入网络，并将最长的工作量证明作为该节点离线期间发生的交易的证明。","link":"/2018/01/21/Blockchain-Notes-Origin/"},{"title":"Blockchain Notes: Part 1","text":"区块链的模型架构区块链基础架构分6层，包括：数据层、网络层、共识层、激励层、合约层、应用层。每层分别完成一个核心功能，各层之间互相配合，实现一个去中心化的信任机制。 数据层：主要描述区块链技术的物理形式。系统设计人员首先建立一个起始节点——创世区块，之后在同样规则下创建的规则相同的区块通过一个链式结构依次相连组成一条主链条。随着运行时间越来越长，新的区块通过验证后不断被添加到主链上，主链也不断增长。时间戳技术保证每个区块顺序相连，哈希函数保证交易信息不被篡改。 网络层：实现区块链网络节点之间的信息交流。区块链网络本质上是P2P网络，每个节点既接收信息，也产生信息，节点间通过维护一个共同的区块链来保护通信。区块链网络中，每个节点都可以创造新的区块，在新区块被创造后会以广播的形式通知其他节点，其他节点会对这个区块进行验证，当全区块链网络中超过51%的用户验证通过后，这个新区块就可以被添加到主链上。 共识层：共识层能让高度分散的节点在去中心化的系统中高效地针对区块数据的有效性达成共识。区块链中比价常用的共识机制主要有：工作量证明、权益证明和股份授权证明三种。 激励层：提供一定的激励措施，鼓励节点参与区块链的安全验证工作。以比特币为例，奖励机制有两种。在比特币总量达到2100万枚之前，奖励机制有：新区块产生后系统奖励的比特币，每笔交易扣除的比特币（即手续费）。当比特币总量达到2100万枚时，新产生区块将不再生成比特币，此时奖励机制主要是每笔交易扣除的手续费。 合约层：主要指各种脚本代码、算法机制及智能合约等。以比特币为例，比特币是一种可编程火币，合约层封装的脚本中规定了比特币的交易方式和过程中涉及到的种种细节。 应用层：封装区块链的各种应用场景和案例。比如基于区块链的跨境支付平台OKLink，以及各种其他五花八门的应用。 区块链的基本类型公有链：公有链是指全世界任何人都可读取、任何人都能发送交易且交易能获得确认，任何人都能参与共识过程的区块链——共识过程决定哪个区块可被添加到区块链中，同时明确当前状态。它有几个特点： 保护用户免受开发者影响访问门槛低所有数据默认公开 私有链：私有链是指其写入权限尽在一个组织手里的区块链，目的是对读取权限或者对外开放权限进行限制。它有以下几个特点： 交易速度非常快：私有链中少量节点具有很高信任度，并不需要每个节点都来验证一个交易，因而交易速度比公有链快很多。为隐私提供更好保障。私有链的数据并不对拥有网络链接的所有人公开。交易成本大幅降低甚至为零：如果一个实体机构控制和处理所有的交易，它就不再需要为工作收取费用。有助于保护其基本的产品不被破坏。银行和传统金融机构使用私有链可以保证其既有利益，已致原有的生态不被破坏。 联盟链：共识过程受到预选节点控制的区块链。比如，对由15个金融机构组成的共同体而言，每个机构都运行着一个节点，为了使每个区块生效需要获得其中半数以上也就是8家机构的确认。区块链可能会允许每个人读取，也可能会受限于参与者走混合路线。联盟链可视为“部分去中心化”，区块链项目R3 CEV就可以认为是联盟链的一种形态。 其他分类说法：许可链指每个节点都需要许可才能加入的区块链系统，私有链和联盟链都属于许可链。随着区块链技术的日益发展，区块链的技术架构不再简单地划分为私有链和公有链，它们之间的界限越来越模糊，于是复杂链和混合链的概念逐渐被提出。 区块链的发展脉络根据区块链科学研究创始人梅兰妮·斯万（Melanie Swan）的观点，区块链技术发展分为三个节点或领域：区块链1.0 / 2.0 / 3.0 区块链 1.0: 以比特币为代表的可编程货币。它更多是指数字货币领域的创新，如货币转移、兑付和支付系统等。 区块链2.0: 基于区块链的可编程金融。它更多涉及一些合约方面的创新，特别是商业合同以及交易方面的创新，比如股票、证券、期货、贷款、清算结算、所谓的智能合约等。 区块链3.0: 区块链在其他行业的应用。更多对应人类组织形式的变革，包括健康、科学、文化和基于区块链的司法、投票等。","link":"/2018/01/22/Blockchain-Notes-Part-1/"},{"title":"Blockchain Notes: Part 2","text":"共识机制的引入在了解共识机制之前，先来看两个古老的引入问题：类两军问题、拜占庭将军问题。 类两军问题： 古代有两个相距很远的军队要传递信息，蓝军派遣一个信使去跟红军说：有本事把意大利炮拿过来！红军收到后回复蓝军说：收到指令。蓝军要给出确认答复：知道你收到指令了！红军继续给出答复：知道你知道我知道指令了！ 拜占庭将军问题： 拜占庭罗马帝国在军事行动中，采取将军投票策略来决定进攻还是撤退，即如果多数人决定进攻，就整体确定进攻策略。但是军队中如果有奸细（将军可能反水、传令官可能误传），如何保证最后投票真实反映忠诚将军的决策？ 拜占庭帝国周围有10个小国，它们饱受拜占庭欺压，却只有同一时间有6个以上国家进攻才有可能打败拜占庭帝国，非则一定战败。 难点在于：古时候军队之间的通信完全依赖于人，如果军队中有奸细，无论是将军反水还是传令官误传，都会是另外9个国家收到假消息，从而造成作战失败。如果你是国王，该如何判断一定会有另外5个以上国家与你并肩作战？毕竟一不小心，就亡国了。 由于类似于以上这样的问题存在，共识的必要性浮现出来。 九种共识机制区块链上的共识机制有多种，但任何一种都不是完美无缺，或者说适用于所有应用场景的。 工作量证明工作量证明（Proof of Work，简称PoW）通常只能从结果证明，因为监测工作过程通常是繁琐且低效的。 比特币在区块的生成过程种使用了PoW机制，一个符合要求的区块哈希值由N个前导零构成，零的个数取决于网络的难度值。要得到合理的区块哈希值需要经过大量的尝试计算，计算时间取决于机器的哈希运算速度。当某个节点提供出一个合理的区块哈希值，说明该节点确实经过了大量的尝试计算，但是并不能得出计算次数，因为寻找合理的哈希值是一个概率事件。当节点拥有占全网n%的算力时，该节点既有n%的概率找到区块哈希值。 PoW依赖机器进行数学运算来获取记账权，资源消耗大、共识机制高、可监管性弱，同时每次达成共识需要全网共同参与运算，性能效率比较低，容错性方便允许全网50%节点出错。 PoW的优点：完全去中心化，节点自由进出。 PoW的缺点：目前比特币已经吸引全球大部分的算力，其他再使用PoW共识机制的区块链应用很难获得相同的算力来保障自身安全；挖矿造成大量的资源浪费；共识达成的周期较长。 使用PoW的项目有：比特币、以太坊的前三个阶段（Frontier前沿、Homestead家园、Metropolis大都会）。以太坊的第四个阶段 Serenity宁静 将采用权益证明机制。 权益证明权益证明（Proof of Stake，简称PoS）由QuantumMechanic2011年在比特币论坛讲座上首先提出，后经Peercoin（点点币）和NXT（未来币）以不同思路实现。 PoS的主要理念是节点记账权的获得难度与节点持有的权益成反比，相比PoW，其在一定程度上减少了数学运算带来的资源消耗，性能也得到了相应的提升，但依然是基于哈希运算，竞争获取记账权的方式，可监管性弱。该共识机制的容错性和PoW相同。它是PoW的一种升级，根据每个节点所占代币的比例和时间，等比例地降低挖矿难度，从而加快找到随机数的速度。 在PoW中，一个用户可能拿1000美元来购买计算机，并加入网络来挖矿以此产生新区块，从而得到奖励。而在PoS中，用户可以拿1000美元购买等价的代币，并把这些代币当作押金放入PoS机制中，这样用户就有机会产生新区块而得到奖励。 总体而言，这个系统中存在一个持币人的集合，他们把手中的代币放入PoS机制中，这样他们就变成验证者。比如对区块链最前面的一个区块而言，PoS算法在验证者中随机选择一个（选择验证者的权重依据他们投入的代币量，比如一个投入押金为1W代币的验证者被选择的概率是一个投入1K代币验证者的10倍），给他权利产生下一个区块。如果在一定时间内，这个验证者没有产生一个区块，则选出第二个验证者代替产生新区块。与PoW一样，PoS以最长的链为准。 随着规模经济（指扩大生产规模引起经济效益增加的现象）的消失，中心化所带来的风险减小了。价值1000万美元的代币带来的回报不多不少，是价值100万美元代币的10倍，不会有人因为负担得起大规模生产工具而得不到成比例的额外回报。 PoS的优点：在一定程度上缩短了共识达成的时间；不再需要大量消耗能源去挖矿。 PoS的缺点：还是需要挖矿，本质上没有解决商业应用的痛点；所有的确认都只是一个概率上的表达，而不是一个确定性的事情，理论上有可能存在其他攻击影响，例如以太坊的DAO攻击事件造成以太坊硬分叉，而ETC随之出现，事实上证明了此次硬分叉的失败。 股份授权证明BitShares（比特股）社区首先提出了股份授权证明（简称DPoS）机制，它与PoS的主要区别在于节点选举若干代理人，由代理人验证和记账，但其合规监管、性能、资源消耗和容错性与PoS相似。类似于董事会投票，持币者投出一定数量的节点，进行代理验证和记账。 DPoS的工作原理如下：每个股东按其持股比例拥有相应的影响力，51%股东投票的结果将是不可逆且有约束力的，其挑战是通过及时而高效的方法达到“51%批准”； 为了达到这个目标，每个股东可以将其投票授予一名代表。获票数最多的前100位代表按既定时间表轮流产生区块。每位代表分配到一个时间段来生产区块。 所有的代表将收到等同于一个平均水平的区块所含交易费的10%作为报酬。如果一个平均水平的区块用100股作为交易费，一位代表将获得一股作为报酬。 网络延迟有可能使某些代表没能及时广播他们的区块，而这将导致区块链分叉。然而，这不太可能发生，因为制造该区块的代表可以与制造该区块前后的区块的代表建立直接连接。建立这种与你之后的代表（也许也包括其后的那名代表）的直接连接是为了确保你能得到报酬。 DPoS的投票模式可以每30秒产生一个新区块，并且在正常的网络条件下，区块链分叉的可能性极其小，即使发生也可以在几分钟内得到解决。执行该模式的基本步骤如下： 成为代表。成为一位代表，你必须在网络上注册你的公钥，并获得一个32位的特有标识符。该标识符会被每笔交易数据的“头部”引用。 授权投票。每个钱包有一个参数设置窗口，在该窗口里用户可以选择一位或更多的代表，并将其分级。一经设定，用户所做的每笔交易将把选票从“输入代表”转移至“输出代表”。一般情况下，用户不会创建专门以投票为目的的交易，因为那将耗费他们一笔交易费。但是在紧急情况下，某些用户可能觉得通过支付费用这一更积极的方式来改变他们的投票是值得的。 保持代表忠诚。每个钱包将显示一个状态指示器，让用户知道他们的代表表现如何。如果他们错过了太多的区块，那么系统将会推荐用户更换一位新的代表。如果任何代表被发现签发了一个无效的区块，那么所有标准钱包将在每个钱包进行更多交易前要求选出一位新代表。 抵抗攻击。在抵抗攻击上，前100位代表所获得的权利是相同的，即每位代表都有一项平等的投票权，因此，无法通过获得超过1%的选票而将权利集中到单一代表上。由于只有100位代表，不难想象一个攻击者可以对每位轮到其生产区块的代表依次进行拒绝服务攻击。幸运的是，由于每位代表的标识是其公钥而非IP地址，这种特定攻击的威胁很容易被减轻。这将使确定DDoS（分布式拒绝服务）攻击目标更为困难。而代表之间的潜在连接将使妨碍他们生产区块变得更为困难。 DPoS的优点：大幅缩小参与验证和记账节点的数量，可以达到秒级的共识验证。 DPoS的缺点：整个共识机制还是依赖于代币，而很多商业应用是不需要代币的。 投注共识投注共识是以太坊下一代的共识机制Casper（鬼马小精灵）引入的一个全新概念，属于PoS。Casper的共识是按区块达成的，而不像PoS那样按链达成。 为了防止验证人在不同的世界中提供不同的投注，我们还有一个简单严格的条款：如果你两次的投注序号一样，或者说你提交了一个无法让Casper依照合约处理的投注，你将失去所有保证金。从这一点我们可以看出，Casper与传统的PoS不同的是，Casper有惩罚机制，这样非法节点通过恶意攻击网络不仅得不到交易费，而且还面临着保证金被没收的风险。 Casper协议下的验证人需要完成出块和投注两个活动。具体如下： 出块是一个独立于其他所有时间而发生的过程，验证人收集交易，当轮到他们的出块时间时，他们就制造一个区块，并签名，然后发送到网络上。投注的过程更为复杂一些，目前Casper默认的验证人策略被设计为模仿传统的拜占庭容错共识：观察其他的验证人如何投注，取33%处的值，向0或1进一步移动。 而客户端确认当前状态的过程是这样的：一开始先下载所有的区块和投注，然后用上面的算法来形成自己的意见，但是不公布意见；它只是简单地按顺序在每个高度进行观察，如果一个区块的概率高于0.5就处理它，否则就跳过它。在处理所有的区块之后，所得到的状态就可以显示为区块链的“当前状态”。客户端还可以给出对于“最终确定”的主观看法：如果高度k之前的每个区块形成的意见高于99.999%或者低于0.001%，那么客户端可以认为前k个区块已经最终确定。 瑞波共识机制瑞波共识算法使一组节点能够基于特殊节点列表形成共识。初始特殊节点列表就像一个俱乐部，要接纳一个新成员，必须由该俱乐部51%的会员投票通过。共识遵循这些核心成员的“51%权利”，外部人员则没有影响力。由于该俱乐部由中心化开始，它将一直是中心化的，而如果它开始腐化，股东们什么也做不了。与比特币及Peercoin一样，瑞波系统将股东们与其投票权隔开，因此，它比其他系统更中心化。 Pool验证池基于传统的分布式一致性技术以及数据验证机制，Pool（联营）验证池是目前行业内大范围使用的共识机制。它的优缺点如下： 优点：不需要代币也可以工作，在成熟的分布式一致性算法（Paxos、Raft）的基础上，实现秒级共识验证。 缺点：去中心化程度不如比特币，更适合多方参与的多中心商业模式。 实用拜占庭容错在分布式计算上，不同的计算机通过信息交换尝试达成共识，但有时候，系统中的协调计算机或者成员计算机可能因系统错误，而交换错误信息，以致影响最终的系统一致性。对于拜占庭将军问题，若根据错误计算机的数量，寻找可能的解决办法，这其实无法找到一个绝对的答案，只可以用来验证一个机制的有效程度。 而拜占庭将军问题的可能解决方法为：在N≥3F+1的情况下，一致性是可能实现的（N为计算机总数，F为有问题的计算机总数）。信息在计算机间互相交换后，各计算机列出所有得到的信息，以大多数的结果作为解决办法。 最早由卡斯特罗和利斯科夫在1999年提出的使用拜占庭容错（PBFT）是第一个得到广泛应用的拜占庭算法。只要系统中有2/3的节点是正常工作的，就可以保证一致性。 使用拜占庭容错算法的总体过程如下：客户端向主节点发送请求调用服务操作，如“”，这里客户端c请求执行操作o，时间戳t用来保证客户端请求只会执行一次。每个由副本节点发给客户端的消息都包含了当前的视图编号，使得客户端能够追踪视图编号，从而进一步推算出当前主节点的编号。客户端通过点对点消息向它自己认为的主节点发送请求，然后主节点自动将该请求向所有备份节点进行广播。 视图编号是连续编号的整数，主节点由公式p=v mod |R|计算得到，这里v是视图编号，p是副本编号，|R|是副本集合的个数。 副本发给客户单的响应为“”，v是视图编号，t是时间戳，i是副本的编号，r是请求执行的结果。 主节点通过广播将请求发送给其他副本，然后就开始执行三个阶段的任务。 预准备阶段。主节点分配一个序列号n给收到的请求，然后向所有备份节点群发预准备消息，预准备消息格式为“&lt;, m&gt;”，这里v是视图编号，m是客户端发送的请求消息，d是请求消息m的摘要。 准备阶段。如果备份节点i接受了预准备消息，则进入准备阶段。在准备的同时，该节点向所有副本节点发送准备消息“”，并且将预准备消息和准备消息写入自己的消息日志。 确认阶段。当“(m, v, n, i)”条件为真的时候，副本i将“”向其他副本节点广播，于是就进入了确认阶段。所有副本都执行请求并将结果发回客户端。客户端需要等待不同副本节点发回相同的结果，作为整个操作的最终结果。 如果客户端没有在有限时间内收到回复，请求将向所有副本节点进行广播；如果该请求已经在副本节点处理过了，副本就向客户端重发一遍执行结果；如果请求没有在副本节点处理过，该副本节点将把请求转发给主节点；如果主节点没有将该请求进行广播，那么就认为主节点失效；如果有足够多的副本节点认为主节点失效，则会触发一次视图变更。 图2-85展示了在没有发生主节点失效的情况下算法的正常执行流程，其中副本0是主节点，副本3是失效节点，而c是客户端。 使用拜占庭容错机制是一种采用“许可投票、少数服从多数”来选举领导者并进行记账的共识机制，该共识机制允许拜占庭容错，允许强监督节点参与，具备权限分级能力，性能更高，耗能更低，而且每轮记账都会由全网节点共同选举领导者，允许33%的节点作恶，容错率为33%。 由于特别适合联盟链的应用场景，实用拜占庭容错机制及其改进算法为目前使用最多的联盟链共识算法，其改进算法为目前使用最多的联盟链共识算法，其改进算法在以下方面进行了调整：修改底层网络拓扑的要求，使用P2P网络；可以动态地调整节点数量；减少协议使用的消息数量。 授权拜占庭容错2016年4月，小蚁公司发布共识算法白皮书，描述了一种通用共识机制——授权拜占庭容错，提出了一种改进的拜占庭容错算法，使其能够适用于区块链系统。授权拜占庭容错算法在使用拜占庭容错算法的基础上，进行了以下改进： 将C/S架构的请求响应模式改进为适合P2P网络的对等节点模式；将静态的共识参与节点改进为可动态进入、退出的共识参与节点；为共识参与节点的产生设计了一套基于持有权益比例的投票机制，通过投票决定共识参与节点（记账节点）；在区块链中引入数字证书，解决了投票中对记账节点真实身份的认证问题。授权拜占庭容错机制的优点：专业化的记账人；可以容忍任何类型的错误；记账由多人协同完成；每一个区块都有最终性，不会分叉；算法的可靠性有严格的数字证明。 授权拜占庭容错机制的缺点：当1/3及以上的记账人停止工作后，系统将无法提供服务；当1/3及以上的记账人联合作恶，且其他所有的记账人被恰好分割为两个网络孤岛时，恶意记账人可以使系统出现分叉，但是会留下密码学证据。 总而言之，授权拜占庭容错机制最核心的一点，就是最大限度地确保系统的最终性，使区块链能够适用于真正的金融应用场景。 Paxos算法这是一种传统的分布式一致性算法，是一种基于选举领导者的共识机制。领导者节点拥有绝对权限，并允许强监督节点参与，其性能高，资源消耗低。所有节点一般有线下准入机制，但选举过程中不允许有作恶节点，不具备容错性。","link":"/2018/01/23/Blockchain-Notes-Part-2/"},{"title":"Bracket Matching Problem With Stack Structure","text":"#include&lt;stdio.h&gt; #include&lt;string.h&gt; #include&lt;stdlib.h&gt; #define MAX 100 #define INIT_STACK_SIZE 10 #define STACKINCREMENT 10 typedef struct{ char *base; char *top; int stacksize; }SqStack;//标准的栈定义 SqStack InitStack(SqStack S); int Process(SqStack S,char expression[],int len); SqStack Push(SqStack S,char c); int StackEmpty(SqStack S); SqStack Pop(SqStack S); void OutPut(SqStack S); int main(void) { SqStack S; char expression[MAX]; int len; printf(&quot;Please input string: /n&quot;); scanf(&quot;%s&quot;,expression); len=strlen(expression);//取栈的长度 S=InitStack(S); //初始化栈 if(Process(S,expression,len))//Process 用来判断字符串 printf(&quot;success./n&quot;); else printf(&quot;failure./n&quot;); return 0; } SqStack InitStack(SqStack S) { if((S.base=(char *)malloc(INIT_STACK_SIZE * sizeof(char)))==NULL) { exit(1); } S.top=S.base; S.stacksize=INIT_STACK_SIZE; return S; } int Process(SqStack S,char expression[],int len) { int i,flag=1; for(i=0;i&lt;=len-1;i++) { switch(expression[i]) { case '(': //如果出现左括号就压入栈 S=Push(S,expression[i]); break; case '[': S=Push(S,expression[i]); break; case '{': S=Push(S,expression[i]); break; case ')': if(!StackEmpty(S) &amp;&amp; *(S.top-1) == '(') //栈不空，且前一个元素是左括号 S=Pop(S); //出栈 else { flag=0; return flag; } break; case ']': if(!StackEmpty(S) &amp;&amp; *(S.top-1) == '[') S=Pop(S); else { flag=0; return flag; } break; case '}': if(!StackEmpty(S) &amp;&amp; *(S.top-1) == '{') S=Pop(S); else { flag=0; return flag; } break; default: flag=0; return flag; }//switch }//for if(S.top == S.base) //栈空 flag=1; else flag=0; return flag; } SqStack Push(SqStack S,char c) { *(S.top++)=c; if(S.top-S.base &gt;= S.stacksize) { if((S.base=(char *)realloc(S.base,(S.stacksize + STACKINCREMENT) * sizeof(char)))==NULL) { exit(1); } S.top=S.base + S.stacksize; S.stacksize+=STACKINCREMENT; } return S; } int StackEmpty(SqStack S) { int flag=0; if(S.top == S.base) flag=1; return flag; } SqStack Pop(SqStack S) { S.top--; return S; }","link":"/2010/11/19/Bracket-Matching-Problem-With-Stack-Structure/"},{"title":"Build MySQL_5.7.30 With Source Code On Mac Catalina","text":"在Mac Catalina上准备好C/C++开发环境后，只需要再安装cmake，就可以编译MySQL源码了。 brew install cmake 下载MySQL源码选择版本5.7.30（计划先看5.x，再看8.0，然后在源码层面做对比） 国内镜像下载地址（搜狐），选择mysql-boost-5.7.30.tar.gz mysql-boot是指包含了C++ Boost库的MySQL源码，对于未在本地安装（或无法安装）Boost库的开发环境较为友好。 进行编译注意cmake参数配置配置前，可以先看看官方文档的CMake配置说明，对于CMake这一工具也能多些了解。 # Specify an installation directory PREFIX_DIR=/data/mysql-5.7.30 cmake \\ -DCMAKE_INSTALL_PREFIX=${PREFIX_DIR} \\ -DMYSQL_DATADIR=${PREFIX_DIR}/data \\ -DSYSCONFDIR==${PREFIX_DIR}/etc \\ -DMYSQL_UNIX_ADDR=${PREFIX_DIR}/etc/mysql.sock \\ -DWITH_MYISAM_STORAGE_ENGINE=1 \\ -DWITH_INNOBASE_STORAGE_ENGINE=1 \\ -DWITH_MEMORY_STORAGE_ENGINE=1 \\ -DDEFAULT_COLLATION=utf8mb4_general_ci \\ -DMYSQL_TCP_PORT=3306 \\ -DENABLED_LOCAL_INFILE=1 \\ -DWITH_PARTITION_STORAGE_ENGINE=1 \\ -DEXTRA_CHARSETS=all \\ -DDEFAULT_CHARSET=utf8mb4 \\ -DENABLE_DOWNLOADS=1 \\ -DWITH_BOOST=./boost 开始构建执行make，生成二进制文件，过程较慢，在2017版MBA 1.8GHz Dual Core i5的CPU下执行了1.3小时。 time make ... make 4125.91s user 289.12s system 95% cpu 1:16:48.86 total","link":"/2020/12/27/Build-MySQL-5-7-30-With-Source-Code-On-Mac-Catalina/"},{"title":"Changes and Expectations","text":"Here I’d like to share some of recent updates of mine. ChangesAfter a year’s endeavor on intelligent database auto-tuning system, to which I’ve devoted all myself since March last year, I’ve grown to be a Mr-know-sth on MySQL database, known each other’s advantages and disadvantages in the team, learned the absolute importance of self-motivation, became a fullstack engineer, and by the way, might possibly get promoted. I met xiaoya, through friends. She is very smart, strong minded, beautiful, and sometimes cuteness is her label. It was she who undesignedly brought embellished sunshine to me. How lucky I was. I’ve got a condo in a prefered district that meets the requirements of my family and myself. This city’s housing price has been rising vigorously since year 2015, it might be a good timing now. Mortgage shall be started paying next month, I gotta work real hard :) ExpectationsIt’s being said that I’ve grown up a lot, I don’t have a specialization field. It’s an urgent truth that I must find out which part in this project at current stage fits me the most, in other words, in which I can do better than anyone else. Then it’s about being nice. There’s a popular game called Werewolf kill, in the game every participator must speak very carefully to convince others believe he/she is the good guy ( not werewolf ). Though it takes quite a lot of logic and performing skills, as a matter of experience, sunniness is the most important to gain trust. And the last one, respect for everyone and what you do. Attitude changes everything, from heaven to hell, from happiness to sorrow, from anything good to bad.","link":"/2017/07/01/Changes-and-Expectations/"},{"title":"Collect 1688 Product Information Using Scrapy And Lazy Loading","text":"使用scrapy懒加载爬取1688商品图片说起 scrapy, 很多人应该都不陌生：它是基于 python 的爬虫框架，编程模型十分简单，极易上手，官方文档在此。 我用它爬取过节目音效文件和一些小网站，都十分简单，后来逛淘宝发现其详情页商品图片使用了懒加载技术：只有在浏览器中纵向滚动条滚动到指定的位置时，页面的图片元素才会被动态加载。注意，在加载之前，图片元素不会出现在页面上。它实质上是由用户出发滚动条事件后，JavaScript 自动生成的。 这样的话，使用最原始的解析网页元素的方式不会取得效果，必须想个方法让 scrapy 可以运行 JavaScript。想到了很久以前一个网站自动化测试工具 selenium, 它让我们能够控制浏览器做一些比如滑动滚动条之类的人工操作。 因此，使用 selenium 打开页面并滑动至页面底部，这样商品详情中的图片就会全部加载出来，然后使用 selector/xpath 选取响应的页面元素，并添加 ImagePipeline，最终达到下载商品详情页图片的效果。以下是用到的关键代码。 # 以下代码使用 python 3.6 及 scrapy 1.4.0 class ProductSpider(scrapy.Spider): name = &quot;Product1688&quot; start_urls = [] def __init__(self, **kwargs): # 加载 chrome driver, 它的下载地址位于 https://sites.google.com/a/chromium.org/chromedriver/ super().__init__(**kwargs) self.driver = webdriver.Chrome('/path/to/your/chromedriver') self.wait = WebDriverWait(self.driver, 10) def parse(self, response): self.driver.get(response.url) # 打开页面后，滑动至页面底部 self.scroll_until_loaded() # 以 xpath 寻找商品名（标题 ) title = self.driver.find_element_by_xpath('//*[@id=&quot;mod-detail-title&quot;]/h1') # 以 xpath 寻找商品主图片 main_images_elements = self.driver.find_elements_by_xpath('//*[@id=&quot;dt-tab&quot;]/div/ul/li/div/a/img') # 以 xpath 寻找商品详情图片 detail_images_elements = \\ self.driver.find_elements_by_xpath('//*[@id=&quot;desc-lazyload-container&quot;]/p/span/strong/img') item = ProductItem() main_images = [] detail_images = [] # 获取商品主图的网络地址，针对商品主图片尺寸的特殊处理 for image in main_images_elements: main_images.append(image.get_attribute('src').replace('60x60.', '')) # 获取商品详情图片的网络地址 for image in detail_images_elements: detail_images.append(image.get_attribute('src')) item['title'] = title.text item['main_image_count'] = len(main_images) item['image_urls'] = main_images + detail_images return item # 模拟浏览器页面滚到页面底部的行为 def scroll_until_loaded(self): check_height = self.driver.execute_script(&quot;return document.body.scrollHeight;&quot;) while True: self.driver.execute_script(&quot;window.scrollTo(0, document.body.scrollHeight);&quot;) try: self.wait.until( lambda driver: self.driver.execute_script(&quot;return document.body.scrollHeight;&quot;) &gt; check_height) check_height = self.driver.execute_script(&quot;return document.body.scrollHeight;&quot;) except TimeoutException: break 随后添加 ImagePipeline 的代码由于网络上资源众多，就不赘述了。以上代码仅供参考，视具体需求可会有变动，能满足需求就好。 最后附上 Repo 地址","link":"/2017/12/19/Collect-1688-Product-Information-Using-Scrapy-And-Lazy-Loading/"},{"title":"Creation, Input, Output Of A Binary Tree","text":"#include &lt;stdio.h&gt;/*实现先序建立、中序遍历*/ #include &lt;malloc.h&gt; #include &lt;stdlib.h&gt; typedef struct BiTNode{ char data; BiTNode *lchild,*rchild; }BiTNode,*BiTree; int InitBiTree(BiTree T){ T=NULL; return 1;//初始化成功 } BiTree CreateBiTree()//先序建立一个二叉树 { char x; //x为根节点 BiTree t; scanf(&quot;%c&quot;,&amp;x);/*输入要创建的根结点值*/ if (x==' ') /* 判断当前子树是否创建完成*/ return NULL; else { t=(BiTree)malloc(sizeof(BiTNode)); t-&gt;data=x; t-&gt;lchild=CreateBiTree(); t-&gt;rchild=CreateBiTree(); } return t; } int CountLeaf(BiTree T)//返回二叉树的叶子节点数目 { if(!T) return 0; else { if(!T-&gt;lchild&amp;&amp;!T-&gt;rchild) return 1; return (CountLeaf(T-&gt;lchild)+CountLeaf(T-&gt;rchild)); } } void InOrderTraverse(BiTree ptr)//这是中序遍历的算法。 { if(ptr) { InOrderTraverse(ptr-&gt;lchild); printf(&quot;%c &quot;,ptr-&gt;data); InOrderTraverse(ptr-&gt;rchild); } } int main() { printf(&quot;构建一个二叉树：/n&quot;); BiTree T =CreateBiTree(); printf(&quot;中序遍历二叉树：/n&quot;); InOrderTraverse(T); printf(&quot;/n&quot;); printf(&quot;二叉树的叶子结点个数为%d/n&quot;,CountLeaf(T)); return 0; } 这样的一个二叉树，输入的顺序应该是：AB[][]CD[][]E[][]。","link":"/2010/12/04/Creation-Input-Output-Of-A-Binary-Tree/"},{"title":"Data Type Mappings Between Golang And PostgreSQL","text":"PostgreSQL数据库中的基本数据类型一共是41种，实际应用中，数据库使用类型、数据库实现类型、Go语言承载类型，如何做一一对应，这下子就清楚了。 数据库基本类型 数据库实现类型 Go语言承载类型 bigint INT8 int64 bigserial INT8 int64 bit(4) BIT interface{} bit varying(4) VARBIT interface{} boolean BOOL bool box BOX interface{} bytea BYTEA []uint8 character(4) BPCHAR interface{} character varying(4) VARCHAR interface{} cidr CIDR interface{} circle CIRCLE interface{} date DATE time.Time double precision FLOAT8 interface{} inet INET interface{} integer INT4 int32 interval INTERVAL interface{} json JSON interface{} jsonb JSONB interface{} line LINE interface{} lseg LSEG interface{} macaddr MACADDR interface{} money MONEY interface{} numeric NUMERIC interface{} path PATH interface{} pg_lsn PG_LSN interface{} point POINT interface{} polygon POLYGON interface{} real FLOAT4 interface{} smallint INT2 int16 smallserial INT2 int16 serial INT4 int32 text TEXT string time without time zone TIME time.Time time with time zone TIMETZ time.Time timestamp without time zone TIMESTAMP time.Time timestamp with time zone TIMESTAMPTZ time.Time tsquery TSQUERY interface{} tsvector TSVECTOR interface{} txid_snapshot TXID_SNAPSHOT interface{} uuid UUID interface{} xml XML interface{}","link":"/2021/08/22/Data-Type-Mappings-Between-Golang-And-PostgreSQL/"},{"title":"Dealing With A MySQL Deadlock Problem Caused By Concurrent DML","text":"记一次由并发DML引起的MySQL死锁场景一次出差中，在客户现场遇到了由并发DML引起的MySQL死锁场景 场景再现首先把场景重现并简化，触发条件：①事务隔离级别为InnoDB默认的REPEATABLE-READ，②有多个Session同时进行事务操作，而事务的内容是先删除一行特定记录然后重新插入该记录。 如下图所示： 数据库show engine innodb status吐出的死锁信息如下： ------------------------ LATEST DETECTED DEADLOCK ------------------------ 2020-12-06 22:14:23 0x70000ca32000 *** (1) TRANSACTION: TRANSACTION 1563, ACTIVE 34 sec inserting mysql tables in use 1, locked 1 LOCK WAIT 3 lock struct(s), heap size 1136, 2 row lock(s) MySQL thread id 21, OS thread handle 123145523712000, query id 86 localhost root update insert into tb values(1) *** (1) HOLDS THE LOCK(S): RECORD LOCKS space id 2 page no 4 n bits 72 index PRIMARY of table `test`.`tb` trx id 1563 lock_mode X Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; *** (1) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 2 page no 4 n bits 72 index PRIMARY of table `test`.`tb` trx id 1563 lock_mode X insert intention waiting Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; *** (2) TRANSACTION: TRANSACTION 1564, ACTIVE 21 sec inserting mysql tables in use 1, locked 1 LOCK WAIT 3 lock struct(s), heap size 1136, 2 row lock(s) MySQL thread id 29, OS thread handle 123145523408896, query id 87 localhost root update insert into tb values(2) *** (2) HOLDS THE LOCK(S): RECORD LOCKS space id 2 page no 4 n bits 72 index PRIMARY of table `test`.`tb` trx id 1564 lock_mode X Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; *** (2) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 2 page no 4 n bits 72 index PRIMARY of table `test`.`tb` trx id 1564 lock_mode X insert intention waiting Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;; *** WE ROLL BACK TRANSACTION (2) 从死锁信息中可以看出，两个事务持有的锁是锁在相同的一块区域上： 0: len 8; hex 73757072656d756d; asc supremum;; 死锁分析上面已经指出了不同的SQL语句会加什么锁，更加详细的内容可以在这里看：15.7.3 Locks Set by Different SQL Statements in InnoDB 我们只看本次场景涉及到的DELETE和INSERT语句。 DELETE语句的锁操作DELETE FROM ... WHERE ... sets an exclusive next-key lock on every record the search encounters. However, only an index record lock is required for statements that lock rows using a unique index to search for a unique row. DELETE FROM ... WHERE ...会在所有满足条件的记录上都设置一个排他的next-key锁，即锁住之前的Gap和待删除的记录。 如果删除的数据比当前最大数据max还大，就会锁住(max, +∞)这个GAP；如果删除的数据比当前最小数据min还小，则会锁住(-∞, min)这个GAP。 不过要注意：如果待搜索字段是用唯一键搜索的，那么只会加一个Index Record Lock。 至于为何会出现+∞和-∞两个概念，请看这里22.2.1.3 The Infimum and Supremum Records INSERT语句的锁操作插入语句由于引入意向锁(Insert Intetion Lock)，会稍微复杂一些。 INSERT sets an exclusive lock on the inserted row. This lock is an index-record lock, not a next-key lock (that is, there is no gap lock) and does not prevent other sessions from inserting into the gap before the inserted row. INSERT 会在待插入记录上加排他锁，它是行锁而非GAP锁，并不会阻塞其他会话在该行之前的GAP插入数据。 Prior to inserting the row, a type of gap lock called an insert intention gap lock is set. This lock signals the intent to insert in such a way that multiple transactions inserting into the same index gap need not wait for each other if they are not inserting at the same position within the gap. 但在插入之前，一个名为Insert Intention Gap Lock的GAP锁会加在待插入区域，本次场景中即为(5,+∞)区域。 GAP锁的作用GAP锁作用是，防止其他事务在这个gap内的插入，但不排斥其他事务在同一个gap内加GAP锁，因此Gap X锁和Gap S锁效果相同。 解决直接解决方法：将事务隔离级别由REPEATABLE-READ改为READ-COMMITTED。 因为在RR隔离级别中，读会加排他锁，而RC隔离级别中读只会加共享锁，这样可以避免出现本次场景中的互锁现象。","link":"/2020/12/06/Dealing-With-A-MySQL-Deadlock-Problem-Caused-By-Concurrent-DML/"},{"title":"Decompile A Java Class File","text":"先给出一段简单的代码如下： public class Test { public static void main(String[] args) { int j = 0; for (int i = 0; i &lt; 7 ; i ++) { j = j++; } System.out.println(j); } } 程序输出为0，有点不解，Google后发现是“Java中间缓存变量机制”，还是不清不楚。所以使用javap对编译后的class文件进行分析。 指令为： javac Test.java javap -c Test 得到如下代码： Compiled from &quot;Test.java&quot; public class Test { public Test(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return // 程序主入口 public static void main(java.lang.String[]); Code: 0: iconst_0 //int型常量值0进栈 1: istore_1 //将栈顶int型数值存入第二个局部变量 2: iconst_0 //int型常量值0进栈 3: istore_2 //将栈顶int型数值存入第三个局部变量 4: iload_2 //将第三个int型局部变量进栈 5: bipush 7 //将一个byte型常量值推送至栈顶 7: if_icmpge 21 //栈顶两整数比较，若结果大于0就跳转到21 10: iload_1 //将第二个int型局部变量进栈 11: iinc 1, 1 //指定int型变量增加指定值 14: istore_1 //将栈顶int型数值存入第二个局部变量 15: iinc 2, 1 //指定int型变量增加指定值 18: goto 4 //跳转4 21: getstatic #2 //获取指定类的静态域，并将其值压入栈顶 24: iload_1 //将第三个int型局部变量进栈 25: invokevirtual #3 //调用实例方法 28: return }","link":"/2014/07/14/Decompile-A-Java-Class-File/"},{"title":"Execute SHELL Scripts On Bunch Of Nodes In Batch","text":"#!/bin/bash #check whether package 'expect' is installed. if rpm -q expect then echo &quot;Good&quot;&gt;/dev/null else echo &quot;You Must install package \\'expect\\'&quot;; exit 0 fi #开始产生exp文件 cat &gt;/tmp/auto.tcl&lt;&lt;eof #!/usr/bin/expect -f set timeout -1 set host [lrange \\$argv 0 0] spawn ssh-copy-id -i /root/.ssh/id_rsa.pub root@\\$host expect &quot;*connecting*&quot; send &quot;yes\\r&quot; expect &quot;*?assword:&quot; send &quot;12341234\\r&quot; expect eof eof chmod 755 /tmp/auto.tcl rm -f /root/.ssh/known_hosts for hostname in `cat cluster`; do expect -f /tmp/auto.tcl $hostname; echo &quot;$hostname&quot;&gt;&gt; ssh_copy_id.log; done 很好的解决了新装Linux机器互相之间需要设置SSH无密码访问(ssh-copy-id)的问题(前提是已经用ssh-keygen产生了公钥私钥)。 这只是expect的一个应用，除此之外，exp也可以用在很多需要机器代替人为交互的地方。 注意： 得到所有机器的IP地址(这是必须)，最好拿到机器名与IP地址的对应(至少应该有一台机器的hosts有)。 如果都是IP地址，那就用IP地址好了，一行一个IP存到一个文件里面，就是代码中的 cluster文件；然后从任一机器处，执行此脚本，执行完毕时得到了该机器与所有机器的单项连接。 单向连接既然有了，通过此单向连接，控制其他所有机器运行此脚本即可。","link":"/2012/07/10/Execute-SHELL-Scripts-On-Bunch-Of-Nodes-In-Batch/"},{"title":"First Time Using CTex","text":"\\documentclass[a4paper,11pt]{article} \\usepackage[tikz]{bclogo} \\usepackage{amsmath,amsthm,amssymb} \\usepackage{color} \\usepackage{CJK} \\title{Introduction to Combinatorics Work 1} \\begin{document} \\maketitle {\\color{red}Caution: You need to add your own name and student number at the end of this file BEFORE you print this out.} \\begin{bclogo}[couleur = blue!30,arrondi = 0.1,logo=\\bccrayon, ombre = true]{Theorem} The number of partitions of n - m into exactly k - 1 parts, none exceeding m, equals the number of partitions of n - k into m - 1 parts, none exceeding k. \\end{bclogo} \\begin{proof} Let us consider the graphical representation of partition n-m into exactly k-1 parts, none exceeding m. We transform the partitions as follows;first we adjoin a new top row of m nodes; then we delete the first column (which now has k nodes); and then we take the conjugate: %first graph \\begin{center} \\begin{tikzpicture} \\draw (.5,.5) node {$\\leq m$}; \\draw (-.5, -.5) node {$k-1 $ }; \\foreach \\x in {0,...,4} \\filldraw (\\x*.25, 0) circle (.5mm); \\foreach \\x in {0,...,3} \\filldraw (\\x*.25, -.25) circle (.5mm); \\foreach \\x in {0,...,2} \\filldraw (\\x*.25, -.5) circle (.5mm); \\foreach \\x in {0,...,2} \\filldraw (\\x*.25, -.75) circle (.5mm); \\foreach \\x in {0,...,1} \\filldraw (\\x*.25, -1) circle (.5mm); \\end{tikzpicture} \\end{center} %downarrow \\begin{center} \\begin{tikzpicture} \\draw (.5,.5) node {$\\Downarrow$}; \\end{tikzpicture} \\end{center} %second graph \\begin{center} \\begin{tikzpicture} \\draw (.5,.5) node {$ m$}; \\draw (-.5, -.5) node {$k $ }; \\foreach \\x in {0,...,5} \\filldraw (\\x*.25, 0) circle (.5mm); \\foreach \\x in {0,...,4} \\filldraw (\\x*.25, -.25) circle (.5mm); \\foreach \\x in {0,...,3} \\filldraw (\\x*.25, -.5) circle (.5mm); \\foreach \\x in {0,...,2} \\filldraw (\\x*.25, -.75) circle (.5mm); \\foreach \\x in {0,...,2} \\filldraw (\\x*.25, -1) circle (.5mm); \\foreach \\x in {0,...,1} \\filldraw (\\x*.25, -1.25) circle (.5mm); \\end{tikzpicture} \\end{center} %downarrow \\begin{center} \\begin{tikzpicture} \\draw (.5,.5) node {$\\Downarrow$}; \\end{tikzpicture} \\end{center} %third graph \\begin{center} \\begin{tikzpicture} \\draw (.5,.5) node {$ m-1$}; \\draw (-.5, -.5) node {$\\leq k $ }; \\foreach \\x in {0,...,4} \\filldraw (\\x*.25, 0) circle (.5mm); \\foreach \\x in {0,...,3} \\filldraw (\\x*.25, -.25) circle (.5mm); \\foreach \\x in {0,...,2} \\filldraw (\\x*.25, -.5) circle (.5mm); \\foreach \\x in {0,...,2} \\filldraw (\\x*.25, -.75) circle (.5mm); \\foreach \\x in {0,...,1} \\filldraw (\\x*.25, -1) circle (.5mm); \\foreach \\x in {0,...,0} \\filldraw (\\x*.25, -1.25) circle (.5mm); \\end{tikzpicture} \\end{center} %downarrow \\begin{center} \\begin{tikzpicture} \\draw (.5,.5) node {$\\Downarrow$}; \\end{tikzpicture} \\end{center} %fourth graph \\begin{center} \\begin{tikzpicture} \\draw (.5,.5) node {$\\leq k$}; \\draw (-.5, -.5) node {$m-1 $ }; \\foreach \\x in {0,...,5} \\filldraw (\\x*.25, 0) circle (.5mm); \\foreach \\x in {0,...,4} \\filldraw (\\x*.25, -.25) circle (.5mm); \\foreach \\x in {0,...,3} \\filldraw (\\x*.25, -.5) circle (.5mm); \\foreach \\x in {0,...,1} \\filldraw (\\x*.25, -.75) circle (.5mm); \\foreach \\x in {0,...,0} \\filldraw (\\x*.25, -1) circle (.5mm); \\end{tikzpicture} \\end{center} We see immediately that this composite tranformation provides a one-to-one correspondence between the two types or partitions considered and consequently the theorem is established.\\\\ Problem solved.\\\\ \\begin{flushright} \\begin{CJK*}{GBK}{song} 你的名字\\\\ 你的学号 \\end{CJK*} \\end{flushright} \\end{proof} \\end{document} 代码如上，东拼西凑加上网查各种格式，总算搞定了一篇还不算难看的homework paper。这一下午，值了。 但是，核心的创新、思想，是没法从网上找到的，要会思考，这就是永远的财富。","link":"/2012/09/20/First-Time-Using-CTex/"},{"title":"Flood Fill Algorithm","text":"The algorithmFlood fill is an algorithm that determines the area connected to a given node in a multi-dimensional array. It is used in MS-Paint program as the “bucket” fill tool to fill connected, same-colored areas with one new color, and in games such as Go and Minesweeper for determining which pieces are cleared. In graphic processing, it is also known as boundary fill to fill a particular bounded area with color. The flood-fill algorithm takes three parameters: a start node, a target color, and a replacement color. The algorithm looks for all nodes in the array that are connected to the start node by a path of the target color and changes them to the replacement color. Strucuring the algorthm can be done in many ways, but they all make use of a queue or stack data structure, explicitly or implicitly. Depending on whether we consider nodes touching at the corners connected or not, we have two variations: eight-way and four-way respectively. ImplementationsStack-based(recursive) flood fill Flood-fill ( node, target-color, replacement-color ): 1.If the color of node is not equal to target-color, return; 2.Set the color of node to replacement-color. 3.Perform Flood-fill ( one step to the west of node, target-color, replacement-color). Perform Flood-fill ( one step to the east of node, target-color, replacement-color). Perform Flood-fill ( one step to the north of node, target-color, replacement-color). Perform Flood-fill ( one step to the south of node, target-color, replacement-color). 4.Return. Queue-based flood fill Flood-fill ( node, target-color, replacement-color ): 1.Set Q to the empty queue. 2.If the color of node is not equal to target-color, return. 3.Add node to the end of Q. 4.For each element n of Q: 5\\. Set the color of n to replacement-color. 6\\. If the color of the node to the west of n is target-color, add that node to the end of Q. If the color of the node to the east of n is target-color, add that node to the end of Q. If the color of the node to the north of n is target-color, add that node to the end of Q. If the color of the node to the south of n is target-color, add that node to the end of Q. 7.Continue looping until Q is exhausted. 8.Return. Queue-based flood fill with overhead optimization Flood-fill ( node, target-color, replacement-color ): 1.Set Q to the empty queue. 2.If the color of node is not equal to target-color, return. 3.Add node to the end of Q. 4.For each element n of Q: 5\\. Set w and e equal to n. 6\\. Move w to the west until the color of the node to the west of w no longer matches target-color. 7\\. Move e to the east until the color of the node to the east of e no longer matches target-color. 8\\. Set the color of nodes between w and e to replacement-color. 9\\. For each node n between w and e: 10\\. If the color of the node to the north of n is target-color, add that node to the end of Q. If the color of the node to the south of n is target-color, add that node to the end of Q. 11.Continue looping until Q is exhausted. 12.Return. Working code in Java version: public void floodFill(BufferedImage image, Point node, Color targetColor, Color replacementColor) { int width = image.getWidth(); int height = image.getHeight(); int target = targetColor.getRGB(); int replacement = replacementColor.getRGB(); if (target != replacement) { Deque&lt;Point&gt; queue = new LinkedList&lt;Point&gt;(); do { int x = node.x; int y = node.y; while (x &gt; 0 &amp;&amp; image.getRGB(x - 1, y) == target) { x--; } boolean spanUp = false; boolean spanDown = false; while (x &lt; width &amp;&amp; image.getRGB(x, y) == target) { image.setRGB(x, y, replacement); if (!spanUp &amp;&amp; y &gt; 0 &amp;&amp; image.getRGB(x, y - 1) == target) { queue.add(new Point(x, y - 1)); spanUp = true; } else if (spanUp &amp;&amp; y &gt; 0 &amp;&amp; image.getRGB(x, y - 1) != target) { spanUp = false; } if (!spanDown &amp;&amp; y &lt; height - 1 &amp;&amp; image.getRGB(x, y + 1) == target) { queue.add(new Point(x, y + 1)); spanDown = true; } else if (spanDown &amp;&amp; y &lt; height - 1 &amp;&amp; image.getRGB(x, y + 1) != target) { spanDown = false; } x++; } } while ((node = queue.pollFirst()) != null); } ```}","link":"/2015/12/06/Flood-Fill-Algorithm/"},{"title":"Grpc Tutorial Step By Step","text":"gRpc入门步骤详解1. 安装Go 1.16、Python 3brew install go@1.16 brew install python3 2. 环境变量和其他配置# pip mirror mkdir -p ~/.pip cat &gt;~/.pip/pip.conf &lt;&lt;EOF [global] index-url = https://mirrors.aliyun.com/pypi/simple/ [install] trusted-host=mirrors.aliyun.com EOF # go path export GOPATH=/Users/apple/go export PATH=$PATH:$GOPATH/bin 3. 安装依赖库# go grpc gen go get -u github.com/golang/protobuf/protoc-gen-go # python grpc gen pip3 install grpcio pip3 install protobuf pip3 install grpcio_tools 4. 准备工程目录结构# prepare project directory mkdir -p grpc-demo cd grpc-demo go mod init grpc-demo # prepare proto-buf directory mkdir -p pb # prepare server directory mkdir -p server #prepare client directory mkdir -p client #prepare service directory mkdir -p service 5. 定义.proto协议文件及生成Go代码# file pb/hello_grpc.proto syntax = &quot;proto3&quot;; package service; option go_package = &quot;.;service&quot;; service Greeter { rpc SayHello (HelloRequest) returns (HelloReply) {} } message HelloRequest { string name = 1; } message HelloReply { string message = 1; } 生成Go版本的协议代码 protoc -I pb/ pb/hello_grpc.proto --go_out=plugins=grpc:service -I 后面指定proto文件存放目录，和proto文件--go_out=plugins=grpc:后面指定生成go代码存放的目录检查文件service/hello_grpc.pb.go是否成功生成 6. 编写Go Server代码及运行Server# file server/server.go package main import ( &quot;context&quot; &quot;fmt&quot; &quot;hello_grpc/service&quot; &quot;net&quot; &quot;google.golang.org/grpc&quot; &quot;google.golang.org/grpc/reflection&quot; ) type server struct{} func (s *server) SayHello(ctx context.Context, in *service.HelloRequest) (*service.HelloReply, error) { return &amp;service.HelloReply{Message: &quot;hello &quot; + in.Name}, nil } type GreeterServer interface { SayHello(context.Context, *service.HelloRequest) (*service.HelloReply, error) } func main() { // listen local port lis, err := net.Listen(&quot;tcp&quot;, &quot;:5000&quot;) if err != nil { fmt.Printf(&quot;fail to listen: %s&quot;, err) return } // create grpc server s := grpc.NewServer() // register function service.RegisterGreeterServer(s, &amp;server{}) reflection.Register(s) err = s.Serve(lis) if err != nil { fmt.Printf(&quot;fail to start server: %s&quot;, err) return } } 运行Server，若发现报错则于项目根目录先执行go mod tidy准备好依赖。 go run server/server.go 7. 编写Go Client代码及测试Client# file client/client.go package main import ( &quot;context&quot; &quot;fmt&quot; &quot;hello_grpc/service&quot; &quot;google.golang.org/grpc&quot; ) func main() { // connect server conn, err := grpc.Dial(&quot;:5000&quot;, grpc.WithInsecure()) if err != nil { fmt.Printf(&quot;fail to connect server: %s&quot;, err) return } defer conn.Close() // create new client c := service.NewGreeterClient(conn) // call rpc function r, err := c.SayHello(context.Background(), &amp;service.HelloRequest{Name: &quot;grpc&quot;}) if err != nil { fmt.Printf(&quot;fail to call rpc function: %s&quot;, err) return } fmt.Printf(&quot;rpc call success: %s&quot;, r.Message) } 运行Client测试 % go run client/client.go rpc call success: hello grpc 8. 编写Python Client并测试Client第一步，生成Python语言协议文件hello_grpc_pb2.py和hello_grpc_pb2_grpc.py mkdir -p python python3 -m grpc_tools.protoc -I pb/ --python_out=python/ --grpc_python_out=python/ pb/hello_grpc.proto 第二步，编写Python Client # file python/client.py import logging import grpc import hello_grpc_pb2 import hello_grpc_pb2_grpc def run(): with grpc.insecure_channel('localhost:5000') as channel: stub = hello_grpc_pb2_grpc.GreeterStub(channel) response = stub.SayHello(hello_grpc_pb2.HelloRequest(name='grpc')) print(&quot;call success: {}!&quot;.format(response.message)) if __name__ == '__main__': logging.basicConfig() run() 第三步，运行Python Client % python3 python/client.py call success: hello grpc! 工程打包下载本站地址","link":"/2021/07/03/Grpc-Tutorial-Step-By-Step/"},{"title":"How Does Namenode Find The Nearest Datanode","text":"问题的全部：客户端每次都首先与Namenode通信，然后与Namenode指定的Datanode直接读或写数据。这里Namenode会找到离Client最近的Datanode交给Client，请问这个步骤是如何实现的？ 根据层次的加深，我有三个不同的答案： 有NameNode确定客户端所在的网络，然后找到一个具有某Blocks的与Client在同一个局域网或机架的数据节点NameNode维护着一个集群的网络拓扑结构，在这个图中可以确定与Client最近的具有某Blocks的数据节点从源码级别来看，NameNode用了什么函数来做这件事情。个人觉得第三个答案是最好的，Talk is cheap, show me the code。 “输入流和输出流是DFSClient实现中最复杂的部分，它们不但需要和名字节点通信，还需要访问数据节点。相比之下，输入流比输出流简单，读数据的过程中，名字节点只提供了两个远程方法，getBlockLocations()和reportBadBlocks()。” 客户端在读取数据前先要向NameNode调用RPC获得BlockLocations，这个过程中NameNode就会提供与客户端最近的数据节点给它。NameNode.getBlockLocations()代码如下： /**org.apache.hadoop.hdfs.server.namenode.NameNode*/ /** {@inheritDoc} */ public LocatedBlocks getBlockLocations(String src, long offset, long length) throws IOException { myMetrics.incrNumGetBlockLocations();//统计计数 return namesystem.getBlockLocations(getClientMachine(), src, offset, length);//获取BlockLocations } /**获取Client的地址*/ private static String getClientMachine() { String clientMachine = NamenodeWebHdfsMethods.getRemoteAddress(); if (clientMachine == null) { //not a web client clientMachine = Server.getRemoteAddress(); } if (clientMachine == null) { //not a RPC client clientMachine = &quot;&quot;; } return clientMachine; } 重点来了。 /** * Get block locations within the specified range. * * @see #getBlockLocations(String, long, long) */ LocatedBlocks getBlockLocations(String clientMachine, String src, long offset, long length) throws IOException { LocatedBlocks blocks = getBlockLocations(src, offset, length, true, true, true);// 1，获取源文件所在的所有Block if (blocks != null) { // sort the blocks 2，打算将这些Block按从近到远进行排序 // In some deployment cases, cluster is with separation of task // tracker // and datanode which means client machines will not always be // recognized // as known data nodes, so here we should try to get node (but not // datanode only) for locality based sort. Node client = host2DataNodeMap.getDatanodeByHost(clientMachine);// 3，查看client是否是DataNode节点 if (client == null) {// 3.1，若不是，则对client进行dns解析生成一个NodeBase List&lt;String&gt; hosts = new ArrayList&lt;String&gt;(1); hosts.add(clientMachine); String rName = dnsToSwitchMapping.resolve(hosts).get(0); if (rName != null) client = new NodeBase(clientMachine, rName); } DFSUtil.StaleComparator comparator = null; if (avoidStaleDataNodesForRead) { comparator = new DFSUtil.StaleComparator(staleInterval); } // Note: the last block is also included and sorted // 4，将所有Block所在位置与Client进行比较，然后按比较结果从小到大排序 for (LocatedBlock b : blocks.getLocatedBlocks()) { // 4.1，使用NetworkTopology实例clusterMap进行排序 clusterMap.pseudoSortByDistance(client, b.getLocations()); if (avoidStaleDataNodesForRead) { Arrays.sort(b.getLocations(), comparator); } } } return blocks; } 排序代码 /** * Get block locations within the specified range. * * @see #getBlockLocations(String, long, long) */ LocatedBlocks getBlockLocations(String clientMachine, String src, long offset, long length) throws IOException { LocatedBlocks blocks = getBlockLocations(src, offset, length, true, true, true);// 1，获取源文件所在的所有Block if (blocks != null) { // sort the blocks 2，打算将这些Block按从近到远进行排序 // In some deployment cases, cluster is with separation of task // tracker // and datanode which means client machines will not always be // recognized // as known data nodes, so here we should try to get node (but not // datanode only) for locality based sort. Node client = host2DataNodeMap.getDatanodeByHost(clientMachine);// 3，查看client是否是DataNode节点 if (client == null) {// 3.1，若不是，则对client进行dns解析生成一个NodeBase List&lt;String&gt; hosts = new ArrayList&lt;String&gt;(1); hosts.add(clientMachine); String rName = dnsToSwitchMapping.resolve(hosts).get(0); if (rName != null) client = new NodeBase(clientMachine, rName); } DFSUtil.StaleComparator comparator = null; if (avoidStaleDataNodesForRead) { comparator = new DFSUtil.StaleComparator(staleInterval); } // Note: the last block is also included and sorted // 4，将所有Block所在位置与Client进行比较，然后按比较结果从小到大排序 for (LocatedBlock b : blocks.getLocatedBlocks()) { // 4.1，使用NetworkTopology实例clusterMap进行排序 clusterMap.pseudoSortByDistance(client, b.getLocations()); if (avoidStaleDataNodesForRead) { Arrays.sort(b.getLocations(), comparator); } } } return blocks; } 至此，这个过程算是大概明白了。但是还需要结合整个Namenode的数据结构来看才行，不然就一叶障目不见泰山了。","link":"/2014/07/21/How-Does-Namenode-Find-The-Nearest-Datanode/"},{"title":"How To Destroy Singleton","text":"Singleton: In software engineering, the singleton pattern is a design pattern that restricts the instantiation of a class to one object. ——Wikipedia 单例的实现方式单例模式主要有以下几种实现方式： 枚举方式 饥饿加载 静态块方式 延迟加载 按需加载 枚举方式在第二版《Effective Java》中Joshua Bloch说，“单元素枚举是实现Singleton的最佳方式”，所有的JVM都支持枚举。这种方式极容易实现，而且不会出现其他方式拥有的“序列化”问题。 public enum Singleton { INSTANCE; public void execute ( String arg // for example ) { // Perform operation here } } 这种方式能实现单例的原因，是Java保证任何Enum变量只被实例化一次。Java枚举值是全局可访问的，因此算是被Class Loader延迟加载的单例。它的缺点可能是枚举类型不太灵活。 饥饿加载如果程序始终需要有一个实例，或者创建实例的开销很大，可以看看饥饿加载，它保证始终都会有一个实例。 public class EagerSingleton { private static final EagerSingleton instance = new EagerSingleton(); private EagerSingleton() { } public static EagerSingleton getInstance() { return instance; } } 这个方法的优势有： 当该类被使用时，实例才被创建。 对getInstance()方法不是使用synchronized限制，也就是说所有线程看到的都是同一个实例，没必要使用锁（开销大）。 final关键字说明实例不会被重新定义，保证实例的“有且只有一个”。 静态块方式与上面的饥饿加载非常类似的一个方式是利用静态块来做一些预处理（例如构造函数异常）。 public class Singleton { private static final Singleton instance; static { try { instance = new Singleton(); } catch (Exception e) { throw new RuntimeException(&quot;Darn, an error occurred!&quot;, e); } } public static Singleton getInstance() { return instance; } private Singleton() { // ... } } 延迟加载本方法使用双重检查，由于一些细微BUG的存在，只能用于J2SE 5.0以后的版本。存在的问题是：在多线程out of order write环境下，instance可能在构造函数Singleton执行前就返回。 public class SingletonDemo { private static volatile SingletonDemo instance = null; private SingletonDemo() { } public static SingletonDemo getInstance() { if (instance == null) { synchronized (SingletonDemo.class) { if (instance == null) { instance = new SingletonDemo(); } } } return instance; } } 另外一个更简单整洁的版本如下，在多线程环境下会有开销大限制并发的问题（锁的开销问题）： public class SingletonDemo { private static volatile SingletonDemo instance = null; private SingletonDemo() { } public static synchronized SingletonDemo getInstance() { if (instance == null) { instance = new SingletonDemo(); } return instance; } } 按需加载这是比以上所有方式都更懒的一种方式，它利用了语言对类初始化的优势，全部JVM适用。 内部类只会在getInstance()被调用之后才被引用，因此这种方式是线程安全的，而且没有用到如synchronized和volatile之类的特殊语言结构。 public class Singleton { private Singleton() { } /** * SingletonHolder is loaded on the first execution of Singleton.getInstance() * or the first access to SingletonHolder.INSTANCE, not before. */ private static class SingletonHolder { private static final Singleton INSTANCE = new Singleton(); } public static Singleton getInstance() { return SingletonHolder.INSTANCE; } } 单例模式的破坏尽管程序员正常使用时并不会突破以上各种实现方式的单例设计，但是利用反射和序列化，我们很容易就可以获得单例类的多个实例。以上除了枚举方式之外，都是将构造函数Private化，从而阻止类外调用构造函数。 反射反射是多种高级语言都有的特性，也是元编程的核心所在。它允许程序员在运行中获取类结构、方法、字段，并修改Accessible属性，从而给各种Trick带来了可能。 下面将列出利用反射破坏单例模式的代码： Constructor constructor = LazySingleton.class.getDeclaredConstructor(); constructor.setAccessible(true); LazySingleton ls1 = (LazySingleton) constructor.newInstance(); LazySingleton ls2 = (LazySingleton) constructor.newInstance(); LazySingleton ls3 = LazySingleton.getInstance(); LazySingleton ls4 = LazySingleton.getInstance(); System.out.println(ls1 == ls2 ? &quot;单例&quot; : &quot;多例&quot;); //输出“多例” System.out.println(ls3 == ls4 ? &quot;单例&quot; : &quot;多例&quot;); //输出“单例” 反序列化从输入输出流中读取Object，如ObjectInputStream.readObject()，实际返回是目标类的readResolve()。如果我们的单例类对该函数没有重写，默认将返回该类的新实例。这就是上面几种实现方式具有的“序列化”问题。 public class InnerLazySingleton implements Serializable { private InnerLazySingleton() { } private static class Inner { static final InnerLazySingleton instance = new InnerLazySingleton(); } public static synchronized InnerLazySingleton getInstance() { return Inner.instance; } } public class Test { public static void main(String[] args) throws IOException, ClassNotFoundException { InnerLazySingleton ils = InnerLazySingleton.getInstance(); ByteArrayOutputStream baos = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(baos); oos.writeObject(ils); ObjectInputStream ois = new ObjectInputStream(new ByteArrayInputStream(baos.toByteArray())); InnerLazySingleton ils2 = (InnerLazySingleton) ois.readObject(); System.out.println((ils == ils2 ? &quot;单例&quot; : &quot;多例&quot;)); //输出“多例” } } 如果在单例类中重写readResolve()，如下，则能达到单例效果。 public class InnerLazySingleton implements Serializable { private InnerLazySingleton() { } private static class Inner { static final InnerLazySingleton instance = new InnerLazySingleton(); } public static synchronized InnerLazySingleton getInstance() { return Inner.instance; } private Object readResolve() { return getInstance(); } } 扩展单例和抽象工厂一起使用单例模式经常和抽象工厂模式一起使用，来创建一个全局可用但使用者不知道其详细类型的资源。合用这两种模式的一个例子就是Java Abstract Window Tookit（AWT）。 java.awt.Toolkit是将各种AWT组件绑定到特定本地实现的一个抽象类，它有一个Toolkit.getDefaultToolkit()工厂方法返回基于平台实现的子类Toolkit。因为AWT只需要一个对象来执行绑定，而且该对象创建代价较高，所以这个Toolkit被实现为单例。 比较常见的是工厂+单例模式，工厂类一般都被实现为单例。","link":"/2014/09/04/How-To-Destroy-Singleton/"},{"title":"Java Performance Extreme Programming","text":"Java性能极限编程(转载)不同位数的Hash算法的碰撞概率 32位的Hash算法，无论使用哪一种，都会存在碰撞。 128位的Hash算法（比如MD5）基本找不到碰撞，但性能不够好。 64位的Hash算法，取值范围42亿的平方，基本找不到碰撞。比如，在值范围量并不超过100万的场景，可以假设唯一。 FNV_1a_64算法FNV算法，其改进版本是fnv_1a，fnv是一种非常简洁并有效的hash算法，具体实现如下： public static long fnv1a_64(String chars) { long hash = 0xcbf29ce484222325L; for (int i = 0; i &lt; chars.length(); ++i) { char c = chars.charAt(i); hash ^= c; hash *= 0x100000001b3L; } return hash; } 在上面的代码看来，可以做增量计算。增量计算可以用来很多场景优化，比如class的fullName由package和simpleName组成，如果已经计算了package的hashCode，就是在此基础上计算fullName的hashCode，不用从头开始计算，这在很多场景可以优化性能。 增量计算的实现public static long fnv1a_64_increment(long basic, char seperator, String chars) { long hashCode = basic; // 补上分隔符的Hash增量 hashCode ^= seperator; hashCode *= 0x100000001b3L; // 补上SimpleName的Hash增量 for (int i = 0; i &lt; chars.length(); ++i) { char ch = chars.charAt(i); hashCode ^= ch; hashCode *= 0x100000001b3L; } return hashCode; } // 先计算package的Hash long pkgHashCode64 = fnv1a_64(&quot;com.alibaba.xxx&quot;); // 增量计算fullName的HashCode long fullNameHashCode64 = fnv1a_64_increment(pkgHashCode64, '.', &quot;MyClassName&quot;); 大小写无关的FNV Hash算法实现在SQL分析等场景，为了更好的用户体验，对象名查找需要大小写不敏感。 public static long fnv1a_64_lower(String key) { long hashCode = 0xcbf29ce484222325L;; for (int i = 0; i &lt; key.length(); ++i) { char ch = key.charAt(i); // 所有大写都转成小写 if (ch &gt;= 'A' &amp;&amp; ch &lt;= 'Z') { ch = (char) (ch + 32); } hashCode ^= ch; hashCode *= 0x100000001b3L; } return hashCode; } 二分查找和fnv_1a_hash_64组合使用String[] strings = { &quot;AVG&quot;, &quot;COUNT&quot;, &quot;MAX&quot;, &quot;MIN&quot;, &quot;STDDEV&quot;, &quot;SUM&quot; }; // 计算hashCodes并且排序 long[] hashCodes = new long[strings.length]; for (int i = 0; i &lt; strings.length; i++) { hashCodes[i] = fnv1a_64_lower(strings[i]); } Arrays.sort(hashCodes); // 建立根据hashCode排序的映射关系 String[] keywords = new String[hashCodes.length]; for (String str : strings) { long hash = FnvHash.fnv1a_64_lower(str); int index = Arrays.binarySearch(hashCodes, hash); keywords[index] = str; } // 根据hashCode二分查找到位置，然后获取 public String getKeyword(long hash) { int index = Arrays.binarySearch(hashCodes, hash); if (index &lt; 0) { return null; } return keywords[index]; } 替换hashCode和equals实现class SQLIdentifierExpr { private String name; private long hashCode64; public SQLIdentifierExpr(String name, long hash_lower){ this.name = name; this.hashCode64 = hash_lower; } @Override public int hashCode() { long value = hashCode64; return (int)(value ^ (value &gt;&gt;&gt; 32)); } @Override public boolean equals(Object obj) { if (!(obj instanceof SQLIdentifierExpr)) { return false; } SQLIdentifierExpr other = (SQLIdentifierExpr) obj; // hashCode64相等就假设相等 return this.hashCode64 == other.hashCode64; } } 应用举例 使用HashCode64替换字符串比较在Druid SQL Parser中，抽象与发分析时，使用了通过比较hashCode64替代字符串比较，极大提升语义分析的性能。 public class SQLExprTableSource extends SQLTableSourceImpl { protected SQLExpr expr; public SQLTableSource findTableSource(long alias_hash) { if (alias_hash == 0) { return null; } if (aliasHashCode64() == alias_hash) { return this; } if (expr instanceof SQLName) { long exprNameHash = ((SQLName) expr).nameHashCode64(); if (exprNameHash == alias_hash) { return this; } } if (expr instanceof SQLPropertyExpr) { long hash = ((SQLPropertyExpr) expr).hashCode64(); if (hash == alias_hash) { return this; } } return null; } } 应用举例 fastjson android版本属性名快速匹配// JSON字符串 {”id”:1001,”name”:”wenshao”} // 类定义 public class Person { public int id; public String name; } // 读取属性名的fnv_1a_hashCode64 public long readNameHash() { long hash = 0x811c9dc5; for (; i &lt; text.length; ++p) { char ch = text.charAt(p); if (ch == '&quot;') break; hash ^= ch; hash *= 0x1000193; } return hash; } // 把fieldName hash读取出来，查找对应的FieldDeserializer // 避免了构造fieldName的字符串，减少内存分配，也提升了fndFieldDeserializer的性能 long nameHash = readNameHash(); FieldDeserializer fieldDeser = fndFieldDeserializer(nameHash); if (fieldDeser != null) { fieldDeser.readValue(); } // 以上为伪码，真实实现会复杂很多 更多应用场景 Druid SQL Parser 在Parser是使用fnv_1a_hash优化Parser性能，并且将hashCode64保存到AST（抽象语法树）节点上，用于加速分析时节点查找。比如 [https://github.com/alibaba/druid/blob/master/src/main/java/com/alibaba/druid/sql/ast/expr/SQLIdentifierExpr.java#L42] Fastjson在parse属性名时使用hashCode加速，比如 https://github.com/alibaba/fastjson/blob/android/src/main/java/com/alibaba/fastjson/parser/JSONLexer.java#L2056 数据库相关的应用的基于Name的查找 类名查找 RPC场景的方法Signature匹配 两个数据表算delta","link":"/2018/05/19/Java-Performance-Extreme-Programming/"},{"title":"Li Assisted Driving in Long Distance Road Trip","text":"今年八月，和家里领导商量之后，购入了一台“理想One”，也就是国产电动汽车里著名的“电动三傻”中“理想汽车”的目前唯一一款产品。 在刚刚过去的国庆假期中，我们驾驶“理想One”从杭州往返一次老家，行驶了总计1600公里，其中辅助驾驶参与近1100公里，参与率近70%，算是深入全方位体验了一次头部国产电动车厂商的长途自驾辅助驾驶。 先说说为什么选择“理想One”。这个问题也可以这么问，“为什么选择增程式混动新能源汽车”。 我曾比较过几种混动技术的差别，也在社交场合多次公开支持增程式混动技术，原因主要是： 1、用户更加倾向于可控的能源补充方式，“可油可电”是“焦虑终结者”。2、纯电里程满足城市通行场景之后，更多的纯电里程是多余的。这一点类似智能手机发布前的超长待机功能，有用，但不是通用场景，不具备广泛铺开的条件。 而增程式混动是“理想汽车”首先具备的差异化竞争力。 此外，偶发的用电紧张，高速上抢夺充电桩、新能源汽车销量排名等新闻，只能更加验证我的观点：未来十年是增程混动应该会是主流。 回到辅助驾驶的话题上，国际自动机工程师学会SAE曾定义过自动驾驶的分级标准（比美国NHTSA的标准更严谨更通用），内容如下： 目前市面上的新能源车型，基本能做到L2，比如高速公路、隧道、高架桥上能使用车道保持+自适应巡航+主动刹车。 那么不少车商宣传的L2.5是什么呢？它表达的是比L2更多的自动驾驶功能点，类似的还有宣传L2.75的。除了车道保持+自适应巡航+主动刹车，还提供了盲区辅助、并线辅助、偏离预警等等。 还有部分厂商宣传达到了L4级别，个人建议不要听信。因为车商放出预设道路的测试视频里，对应到分级标准顶多是L3，而L3的前提便是只在预设道路中有效，还很难在我们日常的场景中应用。L4就更加别提了，T家的FSD从放出来的视频看够优秀吧？目前都还是Beta测试阶段，而在国内，FSD更是无法使用的。 进入正题，说说长途自驾使用辅助驾驶的体验。这里只描述个人体验，毕竟不是标准化实验，对外的参考价值不大。 在高速路这样路况好、标线清晰、路上车辆不多的场景下，通过释放驾驶员的双脚、双手及部分感官，可以大幅度降低驾驶员的疲劳感。但是，这仅仅在路上车辆较少、且其他人开车都比较规矩的情况下才能做到，所以辅助驾驶的用户手册里会要求驾驶员的手不能长时间离开方向盘，以便于及时处理突发情况。 亲测在高速路上一些施工区域和划线混乱区域，辅助驾驶100%会出错。 另外高速上常见的穿插式超车场景，辅助驾驶中的车子会因为感知前方加塞而频繁加速、减速，也不适合开启辅助驾驶。 一场长途自驾，要顾及的除了自己的疲劳感，还有乘坐人员的感受和安全，以及不要给路上其他车添堵。如果说驾驶员在其中付出精力是100%，那么辅助驾驶带来的是5%的精力节约吧。","link":"/2021/10/10/Li-Assisted-Driving-in-Long-Distance-Road-Trip/"},{"title":"Longest Common Substring","text":"Longest Common Substring问题描述如下： 给定一个query和一个text，均由小写字母组成。要求在text中找出以同样的顺序连续出现在query中的最长连续字母序列的长度。例如， query为”acbac”，text为”acaccbabb”，那么text中的”cba”为最长的连续出现在query中的字母序列，因此，返回结果应该为其长度3。请注意程序效率。 假设text长为T，query长为Q。 Brute Force很多人的第一想法就是罗列出query和text的所有子串，挨个比较，取出二者都有的子串，最后返回最长的子串长度。 算法的时间复杂度将为O(T^2 * Q^2) int get_max_sub_bf(const char* text, const char* query) { // Time O(T^2 * Q^2) int T = strlen(text); int Q = strlen(query); int max = -1; int flag; for (int i = 0; i &lt; T; i++) { for (int ii = 0; ii &lt; T - i; ii++) { //text[i, ii+i] for (int j = 0; j &lt; Q; j++) { for (int jj = 0; jj &lt; Q - j; jj++) { // query[j, jj+j] if(jj==ii) { flag=1; for(int k = 0;k &lt; ii; k++) { if(text[i+k]!=query[j+k]) { flag = 0; break; } } if(flag &amp;&amp; (max&lt;ii)) { max=ii; } } } } } } return max; } Dynamic Programming典型的以空间换时间，但是用到了子字符串的相关规律。 假设dp[i][j]是字符串text[0…i]和query[0…j]的最长公共子串长度。 那么有：若text[i]==query[j]，则dp[i][j]=dp[i-1][j-1] + 1。 算法复杂度：时间O(T * Q), 空间O(T * Q)。 int get_max_sub_dp(const char* text, const char* query) { // Time O(T*Q) // Space O(T*Q) int T = strlen(text); int Q = strlen(query); int dp[T][Q]; for(int i = 0 ; i &lt; T; i ++) { for(int j = 0; j &lt; Q; j ++) { dp[i][j] = 0; } } // dp[i][j]表示T[0...i]和Q[0...j]的最大公共子串长度 int max = -1; for (int i = 1; i &lt; T; i++) { for (int j = 1; j &lt; Q; j++) { if (text[i - 1] == query[j - 1]) { dp[i][j] = dp[i-1][j-1] + 1; if(max&lt;dp[i][j]) { printf(&quot;dp[%d][%d]\\n&quot;, i, j); max=dp[i][j]; } } } } return max; } 测试与比较首先需要有测试用的text和query，我在Linux环境用Shell命令生成随机字符串。 rm -f in.dat ####生成长度为1000的text字符串到in.dat第一行 tr -cd '[:alnum:]' &lt; /dev/urandom | fold -w1000 | head -n1 &gt; in.dat ####生成长度为100的query字符串到in.dat第一行 tr -cd '[:alnum:]' &lt; /dev/urandom | fold -w100 | head -n1 &gt;&gt; in.dat 编写C代码，读取text和query，用两种方法进行测试，在标准输出打印所用时间。 #include &lt;sys/time.h&gt; #include &lt;stdlib.h&gt; #include &lt;stdio.h&gt; #include &lt;math.h&gt; #include &lt;time.h&gt; int timeval_subtract(struct timeval *result, struct timeval *t2, struct timeval *t1) { long int diff = (t2-&gt;tv_usec + 1000000 * t2-&gt;tv_sec) - (t1-&gt;tv_usec + 1000000 * t1-&gt;tv_sec); result-&gt;tv_sec = diff / 1000000; result-&gt;tv_usec = diff % 1000000; return (diff&lt;0); } void timeval_print(struct timeval *tv) { char buffer[30]; time_t curtime; printf(&quot;%ld.%06ld&quot;, tv-&gt;tv_sec, tv-&gt;tv_usec); curtime = tv-&gt;tv_sec; strftime(buffer, 30, &quot;%m-%d-%Y %T&quot;, localtime(&amp;curtime)); printf(&quot; = %s.%06ld\\n&quot;, buffer, tv-&gt;tv_usec); } int main(int argc, char* argv[]) { struct timeval tvBegin, tvEnd, tvDiff; char *text= NULL; char *query= NULL; FILE *in = fopen(&quot;in.dat&quot;,&quot;r&quot;); size_t len=0; if(in==NULL) return -1; getline(&amp;text, &amp;len, in); getline(&amp;query, &amp;len, in); printf(&quot;%s&quot;, text); printf(&quot;%s&quot;, query); fclose(in); // dp gettimeofday(&amp;tvBegin, NULL); printf(&quot;lcs %d\\n&quot;, get_max_sub_dp(text,query)); gettimeofday(&amp;tvEnd, NULL); timeval_subtract(&amp;tvDiff, &amp;tvEnd, &amp;tvBegin); printf(&quot;dp time elapsed %ld.%06ld s\\n&quot;, tvDiff.tv_sec, tvDiff.tv_usec); // br gettimeofday(&amp;tvBegin, NULL); printf(&quot;lcs %d\\n&quot;, get_max_sub_bf(text,query)); gettimeofday(&amp;tvEnd, NULL); timeval_subtract(&amp;tvDiff, &amp;tvEnd, &amp;tvBegin); printf(&quot;bf time elapsed %ld.%06ld s\\n&quot;, tvDiff.tv_sec, tvDiff.tv_usec); return 0; } 编译代码gcc common_sub_str.c -o common_sub_str 运行./common_sub_str 得出结果： dp time elapsed 0.002574 s bf time elapsed 5.522046 s 实验重复两次，都是得到dp的时间效率高出bf近2000倍。","link":"/2014/09/11/Longest-Common-Substring/"},{"title":"My Implementation Of Apriori Algorithm","text":"项目需要分布式的代码，于是先看了下算法思想，自己学着单机实现了一下。复杂度比较高，其中有两个递归实现。由于Java提供的Set都没有实现Comparable接口，所以自定义了一个Set叫做SimpleSet，其中实现了hashCode, equals, compareTo等函数。 以下代码完全没有指导意义，只作为个人工作记录。 代码如下： SimpleSet.java import java.io.*; import java.util.*; public class SimpleSet&lt;T&gt; implements Set, Comparable{ static int i =0; Set&lt;T&gt; set = new HashSet&lt;T&gt;(); public SimpleSet() { } public SimpleSet(SimpleSet ss) { this.set.addAll(ss.set); } public int hashCode() { int type = this.getClass().hashCode(); int code = 0; for(T t : this.set) { code += (type*31 + t.hashCode()); } return code; } public boolean equals(Object obj) { boolean flag = false; if(obj instanceof SimpleSet) { SimpleSet&lt;?&gt; s = (SimpleSet&lt;?&gt;)obj; if( (s.size()==this.size()) &amp;&amp; s.containsAll(this.set)&amp;&amp; this.set.containsAll(s)) { flag = true; } } return flag; } public String toString() { if (this.set.size() == 0) { return &quot;&quot;; } else { StringBuilder str = new StringBuilder(); str.append(&quot;[&quot;); for (T t : this.set) { str.append(t.toString() + &quot;,&quot;); } String result = str.substring(0, str.length() - 1); result += &quot;]&quot;; return result; } } @Override public boolean add(Object e) { return this.set.add((T) e); } @Override public boolean addAll(Collection c) { return this.set.addAll(c); } @Override public void clear() { this.set.clear(); } @Override public boolean contains(Object o) { return this.set.contains(o); } @Override public boolean containsAll(Collection c) { return this.set.containsAll(c); } @Override public boolean isEmpty() { return this.set.isEmpty(); } @Override public Iterator iterator() { return this.set.iterator(); } @Override public boolean remove(Object o) { return this.set.remove(o); } @Override public boolean removeAll(Collection c) { return this.set.removeAll(c); } @Override public boolean retainAll(Collection c) { return this.set.retainAll(c); } @Override public int size() { return this.set.size(); } @Override public Object[] toArray() { return this.set.toArray(); } public String[] toArray(String[] a) { return this.set.toArray(a); } @Override public Object[] toArray(Object[] a) { // TODO DO NOT USE return null; } @Override public int compareTo(Object o) { return this.hashCode()-o.hashCode(); } } Apriori.java import java.util.*; import java.io.*; public class Apriori { /** * @param args */ static Map&lt;String, Integer&gt; data = new HashMap&lt;String, Integer&gt;(); static List&lt;SimpleSet&lt;String&gt;&gt; lineSet = new ArrayList&lt;SimpleSet&lt;String&gt;&gt;(); // all constructible combinations static Map&lt;SimpleSet&lt;String&gt;, Integer&gt; dataSet = new HashMap&lt;SimpleSet&lt;String&gt;, Integer&gt;(); static final int MIN_SUP = 1; static final float MIN_CONF = 0.1f; static BufferedWriter bw1= null; static BufferedWriter bw2 = null; public static void main(String[] args) throws IOException { bw1= new BufferedWriter(new FileWriter(&quot;FrequentPatterns.txt&quot;, true)); bw2 = new BufferedWriter(new FileWriter(&quot;AssociationRules.txt&quot;, true)); long startTime = System.currentTimeMillis(); String srcFile = &quot;Test.txt&quot;; try { data = buildData(srcFile); } catch (IOException e) { e.printStackTrace(); } Map&lt;SimpleSet&lt;String&gt;, Integer&gt; result = getF1Set(data); Map&lt;SimpleSet&lt;String&gt;, Integer&gt; maxFICMap = new HashMap&lt;SimpleSet&lt;String&gt;, Integer&gt;(); int i = 1; do { printCandidate(result, i); i++; maxFICMap = result; result = recurseGen(result); } while (result.size() &gt; 0); System.out.println(&quot;---------------------------Frequent Patterns---------------------------\\n&quot;); long endTime1 = System.currentTimeMillis(); System.out.println((endTime1 - startTime)+&quot; ms&quot;); //System.out.println(dataSet); //System.out.println(maxFICMap); List&lt;SimpleSet&lt;String&gt;&gt; maxFI = new ArrayList&lt;SimpleSet&lt;String&gt;&gt;( maxFICMap.keySet()); System.out.println(&quot;---------------------------Association Rule---------------------------\\n&quot;); calcAndPrintAsso(maxFI); long endTime2 = System.currentTimeMillis(); System.out.println((endTime2 - endTime1)+&quot; ms&quot;); System.out.println(&quot;----------------------------Consumed Time----------------------------\\n&quot;); long endTime3 = System.currentTimeMillis(); System.out.println((endTime3 - startTime)+&quot; ms&quot;); bw1.close(); bw2.close(); } private static void calcAndPrintAsso(List&lt;SimpleSet&lt;String&gt;&gt; maxFI) throws IOException { for (SimpleSet&lt;String&gt; oneBigSet : maxFI) { if (oneBigSet.size() &gt; 1) { if (dataSet.containsKey(oneBigSet)) { int numerator = dataSet.get(oneBigSet); List&lt;SimpleSet&lt;String&gt;&gt; subs = getSubsets(oneBigSet); for (SimpleSet&lt;String&gt; sub : subs) { if (dataSet.containsKey(sub)) { int denominator = dataSet.get(sub); float confidence = (float) numerator / denominator; bw2.write(sub.toString() + &quot; --&gt; &quot; + oneBigSet.toString() + &quot; conf=&quot; + confidence); bw2.newLine(); } } calcAndPrintAsso(subs); } } } } private static void printCandidate(Map&lt;SimpleSet&lt;String&gt;, Integer&gt; result, int i) throws IOException { int size = result.size(); bw1.write(i + &quot;-item set (num: &quot; + size + &quot;):\\n&quot;); bw1.write(result.toString()); bw1.newLine(); } private static Map&lt;SimpleSet&lt;String&gt;, Integer&gt; recurseGen( Map&lt;SimpleSet&lt;String&gt;, Integer&gt; preMap) { List&lt;SimpleSet&lt;String&gt;&gt; keyList = new ArrayList&lt;SimpleSet&lt;String&gt;&gt;(); keyList.addAll(preMap.keySet()); Map&lt;SimpleSet&lt;String&gt;, Integer&gt; result = new HashMap&lt;SimpleSet&lt;String&gt;, Integer&gt;(); int preSize = keyList.size(); for (int i = 0; i &lt; preSize - 1; i++) { for (int j = i + 1; j &lt; preSize; j++) { SimpleSet&lt;String&gt; key1 = keyList.get(i); SimpleSet&lt;String&gt; key2 = keyList.get(j); String[] pre1 = key1.toArray(new String[0]); String[] pre2 = key2.toArray(new String[0]); if (maybeLinkable(pre1, pre2)) { SimpleSet&lt;String&gt; superSet = new SimpleSet&lt;String&gt;(); int count = 0; for (String str : pre1) { superSet.add(str); } superSet.add(pre2[pre2.length - 1]); if (!shouldCut(keyList, superSet)) { count = checkSup(superSet); if (count &gt;= MIN_SUP) { result.put(superSet, count); dataSet.put(superSet, count);// construct global // reference } } } } } return result; } private static int checkSup(SimpleSet&lt;String&gt; superSet) { int count = 0; for (SimpleSet&lt;String&gt; line : lineSet) { if (line.containsAll(superSet)) { count++; } } return count; } private static boolean shouldCut(List&lt;SimpleSet&lt;String&gt;&gt; preKeyList, SimpleSet&lt;String&gt; superSet) { boolean flag = false; List&lt;SimpleSet&lt;String&gt;&gt; subsets = getSubsets(superSet); for (SimpleSet&lt;String&gt; s : subsets) { if (!preKeyList.contains(s)) { flag = true; break; } } return flag; } private static List&lt;SimpleSet&lt;String&gt;&gt; getSubsets(SimpleSet&lt;String&gt; superSet) { List&lt;SimpleSet&lt;String&gt;&gt; subsets = new ArrayList&lt;SimpleSet&lt;String&gt;&gt;(); String[] superList = superSet.toArray(new String[0]); for (int i = 0; i &lt; superList.length; i++) { SimpleSet&lt;String&gt; copyOfSuperSet = new SimpleSet&lt;String&gt;(superSet); copyOfSuperSet.remove(superList[i]); /* * SimpleSet&lt;String&gt; temp = new SimpleSet&lt;String&gt;(); * * for (int j = 0; j &lt; superList.length; j++) { if (i != j) { * temp.add(superList[i]); } } */subsets.add(copyOfSuperSet); } return subsets; } private static boolean maybeLinkable(String[] pre1, String[] pre2) { boolean flag = true; int size1 = pre1.length; int size2 = pre2.length; if (size1 == size2) { if (size1 != 0) { for (int i = 0; i &lt; size1 - 1; i++) { if (!pre1[i].equals(pre2[i])) { flag = false; break; } } if (pre1[size1 - 1].equals(pre2[size2 - 1])) { flag = false; } } else { flag = false; } } else { flag = false; } return flag; } private static HashMap&lt;SimpleSet&lt;String&gt;, Integer&gt; getF1Set( Map&lt;String, Integer&gt; srcdata) { HashMap&lt;SimpleSet&lt;String&gt;, Integer&gt; f1Set = new HashMap&lt;SimpleSet&lt;String&gt;, Integer&gt;(); for (String str : srcdata.keySet()) { int count = srcdata.get(str); if (count &gt;= MIN_SUP) { SimpleSet&lt;String&gt; set = new SimpleSet&lt;String&gt;(); set.add(str); f1Set.put(set, count);// Be Strict ! dataSet.put(set, count); } } return f1Set; } static Map&lt;String, Integer&gt; buildData(String... files) throws IOException { Map&lt;String, Integer&gt; tdata = new HashMap&lt;String, Integer&gt;(); BufferedReader br = new BufferedReader(new FileReader(files[0])); String line; String[] col; while ((line = br.readLine()) != null) { SimpleSet&lt;String&gt; line_items = new SimpleSet&lt;String&gt;(); col = line.split(&quot; &quot;); for (String str : col) { line_items.add(str); if (!tdata.containsKey(str)) { tdata.put(str, 1); } else { int i = tdata.get(str); tdata.put(str, i + 1);// TODO } } lineSet.add(line_items); } br.close(); return tdata; } }","link":"/2013/09/14/My-Implementation-Of-Apriori-Algorithm/"},{"title":"Oolong Caused By The Annotation Lazy And Schedule","text":"遇到了Spring中一个由于初始化顺序不同导致的问题。 和同学一起Spring Scheduler写了一个包含定时任务的Component，简化如下： package hello; import org.springframework.context.annotation.Lazy; import org.springframework.scheduling.annotation.Scheduled; import org.springframework.stereotype.Component; @Component public class Component1 { // SpringApplicationContext是一个查找Bean的工具类 SomeBean someBean = SpringApplicationContext.getBean(SomeBean.class); @Scheduled(fixedRate = 1000L) public void schedule() { // 执行业务逻辑 someBean.doSomething(); System.out.println(&quot;Scheduled&quot;); } } 由于SomeBean和Component1的实例化顺序不确定，SomeBean晚于Component1初始化的情况下，SpringApplicationContext.getBean(SomeBean.class)会失败，这样就不得不考虑让这一步骤延后，以避免初始化失败。 于是想到了使用Lazy注解延迟Component1的实例化。使用Lazy注解后，Component1将不会在ApplicationContext启动时提前被实例化，而是第一次向容器通过getBean索取bean时实例化的。 @Lazy @Component public class Component1 { 以为这样就解决了应用启动失败的问题，谁知这样导致schedule定时任务不生效了。 仔细看了@Scheduled注解的工作原理后，发现它的核心类是ScheduledAnnotationBeanPostProcessor。其中的核心方法是postProcessAfterInitialization，负责@Schedule注解的扫描，构建ScheduleTask。而这个类实现的是BeanPostProcessor接口，仅在类被实例化之后才被调用。 原因缕清楚了，使用了@Lazy注解的Component1没有在任何其他地方被显式实例化，因为无法被扫描到其内部的@Scheduled注解，观察到的结果就是：定时任务不生效。 有了这些分析后就好办了：既让Component1实例化，使得@Scheduled生效，又让SomeBean能够被找到。 最终使用了如下写法： package hello; import org.springframework.context.annotation.Lazy; import org.springframework.scheduling.annotation.Scheduled; import org.springframework.stereotype.Component; @Component public class Component1 { @Scheduled(fixedRate = 1000L) public void schedule() { // SpringApplicationContext是一个查找Bean的工具类 SomeBean someBean = SpringApplicationContext.getBean(SomeBean.class); // 执行业务逻辑 someBean.doSomething(); System.out.println(&quot;Scheduled&quot;); } }","link":"/2017/07/31/Oolong-Caused-By-The-Annotation-Lazy-And-Schedule/"},{"title":"Ostrich Syndrome","text":"“鸵鸟心态”是一种逃避现实的心理，也是一种不敢面对问题的懦弱行为。心理学通过研究发现，现代人面对压力大多会采取回避态度，明知问题即将发生也不去想对策，结果只会使问题更趋复杂、更难处理。就像鸵鸟被逼得走投无路时，就把头钻进沙子里。 鸵鸟心态是一种消极的人生态度，是一种因为自身能力、环境条件等的不足，进而逃避、绕开问题的懦弱心理。咱们的文化里有两句谚语：”眼不见心不烦”、”掩耳盗铃”。 生活中可以找到真实的案例，例如”老板交代的项目很困难，请求调换一个普通项目来做”、”无法融入新的工作环境，刚进入立即打算转岗”、”工作堆成山怎么也做不完，干脆该干嘛干嘛，按正常节奏走好了”等等。这些案例都有几个共同点： 首先，对象处于有压力甚至高压的环境之中； 其次，对象（认为）自身能力无法克服当前的困境； 最后，对象选择无视该压力（并且忽略这么做的后果）； （以上括号中的文字可能存在） 显然，按照这样的状态推演，我们可能解决不了这个困难。 但是，这种主动逃避的态度会让以后的我们缺乏本来应有的机会，原因不外乎两个： 能力方面：没有锻炼，没有沉积，没有提升，没能力应对以后的挑战； 态度方面：给人们一种不积极、不可靠的形象，没机会获得挑战资格。 但这不是绝对的。 人性之复杂，难以想象。 努力就好了。","link":"/2016/11/02/Ostrich-Syndrome/"},{"title":"Pattern Search Algorithm - KMP","text":"模式搜索算法KMP为了说明算法细节，我们举一个实际例子来说明。假设W=”ABCDABD”，S=”ABC ABCDAB ABCDABCDABDE”。算法的状态在任意时刻都只由两个整数决定： m表示S中可能匹配W的起始位置i表示W中正在比较的字符下标 每一步中，我们比较S[m+i]和W[i]，如果相等则继续下一个字符。1，从第一个字符开始匹配，因B与A不相等，故后移一位。2，从第一个字符开始匹配，B与A不相等，再次后移。3，直到有一个字符相同，继续往下比较，直到发现不同。4，此时该后移多少？应该后移到如图位置。部分匹配值在后面说明。移动位数 = 已匹配的字符数 - 对应的部分匹配值根据相同原理，继续进行比较。直到最后i一直匹配到W的最后一位，匹配成功。 6，下面介绍部分匹配值是如何计算的。首先，有两个概念：“前缀”和“后缀”。有如下定义：前缀指除了最后一个字符以外，一个字符串的全部头部组合；“后缀”指除了第一个字符以外，一个字符串的全部尾部组合。在此基础上，“部分匹配值”就是“前缀”和“后缀”中最长的公共元素的长度。以ABCDABD为例： “A”的前缀和后缀都为空集，共有元素的长度为0；“AB”的前缀为[A]，后缀为[B]，共有元素的长度为0；“ABC”的前缀为[A, AB]，后缀为[BC, C]，共有元素的长度0；“ABCD”的前缀为[A, AB, ABC]，后缀为[BCD, CD, D]，共有元素的长度为0；“ABCDA”的前缀为[A, AB, ABC, ABCD]，后缀为[BCDA, CDA, DA, A]，共有元素为”A”，长度为1；“ABCDAB”的前缀为[A, AB, ABC, ABCD, ABCDA]，后缀为[BCDAB, CDAB, DAB, AB, B]，共有元素为”AB”，长度为2；“ABCDABD”的前缀为[A, AB, ABC, ABCD, ABCDA, ABCDAB]，后缀为[BCDABD, CDABD, DABD, ABD, BD, D]，共有元素的长度为0。 “部分匹配”实质是字符串头部和尾部可能有重复的串，搜索移动的时候，我们能通过此规则跳过一定的字符，达到减小复杂度的目的。 KMP算法的效率算法的两部分“计算部分匹配值”和“寻找匹配串”，复杂度分别为O(n)和O(k)，因此算法整体复杂度为O(n+k)。这项复杂度不因W和S中的重复模式数目而改变。 最后放上实现代码 public class KMP { // kmp算法实现，返回所有匹配处的下标 static ArrayList&lt;Integer&gt; kmp_search(char[] target, char[] pattern) { if (target == null || pattern == null) { throw new IllegalArgumentException(&quot;null char array&quot;);// null param } if (target.length == 0 || pattern.length == 0 || pattern.length &gt; target.length) { throw new IllegalArgumentException(&quot;invalid parameter&quot;);// err return } int[] pmt = get_pmt(pattern);//获取部分匹配数组 ArrayList&lt;Integer&gt; kmpResult = new ArrayList&lt;Integer&gt;(); int S = target.length;// target 元素个数 int W = pattern.length;// pattern 元素个数 int delta;//表示patter该往后移动的位数 int m;// m表示target中可能匹配W的起始位置 int i;// i表示pattern中正在比较的字符下标 for (m = 0; m &lt; S - W + 1; m += delta) { delta = 1; for (i = 0; i &lt; W; i++) { if (pattern[i] != target[m + i]) { if (i != 0) delta = (i - pmt[i - 1]); break; } } if (i == W) { kmpResult.add(m); } } return kmpResult; } // 生成部分匹配表 static int[] get_pmt(char[] pattern) { int len = pattern.length; int[] pmt = new int[len]; int i, j, k, max_cc; for (i = 0; i &lt; len; i++) { max_cc = 0; // prefix pattern[0...j] // postfix pattern[k...i] for (j = 0; j &lt; i; j++) { for (k = 0; k &lt;= j; k++) { if (pattern[k] != pattern[i - j + k]) { break; } } if (k == j + 1) max_cc = Math.max(max_cc, k); } pmt[i] = max_cc; } return pmt; } public static void main(String[] args) { // 测试 char[] target = &quot;AABAACAADAABAAABAA&quot;.toCharArray(); char[] pattern = &quot;ABAA&quot;.toCharArray(); System.out.println(kmp_search(target, pattern)); } }","link":"/2014/09/03/Pattern-Search-Algorithm-KMP/"},{"title":"Play MP3 File With mciSendString","text":"首先，包含头文件 #include&lt;mmsystem.h&gt;//使用mciSendString所要包含的头文件下面的实现代码： case IDC_OPEN: { TCHAR fileName[]=&quot;E://KuGou//飞儿乐团 - 把爱放开.mp3&quot;; TCHAR shortName[MAX_PATH]; GetShortPathName(fileName,shortName,sizeof(shortName)/sizeof(TCHAR)); TCHAR cmd[MAX_PATH+10]; wsprintf(cmd,&quot;play %s&quot;,shortName); mciSendString(cmd,&quot;&quot;,NULL,NULL); }","link":"/2010/11/13/Play-MP3-File-With-mciSendString/"},{"title":"Pointers And References","text":"指针与引用的区别有： 非空区别。指针可以为空，而引用不能。 合法性区别。在使用之前指针必须验证合法性，而引用则不用。 可修改区别。指针可以被重新赋值以指向不同的对象，而引用则总是在初始化时就固定地指向某对象，过后不可修改，但指向的对象内容可以更改。 应用区别。两种情况下应使用指针：1，存在不指向任何对象的可能；2，需要在不同时刻指向不同的对象。如果总是指向一个对象且一直不变，则使用引用。 代码实例： #include &lt;iostream&gt; using namespace std; int main() { int iv; int iv2 = 1024; int iv3 = 999; int &amp;reiv; // 编译错误，引用声明时未初始化 int &amp;reiv2 = iv; int &amp;reiv3 = iv; int *pi; *pi = 5;//运行时错误，欲修改未知地址的内容 cout &lt;&lt; pi &lt;&lt; endl; pi = &amp;iv3; cout &lt;&lt; pi &lt;&lt; endl; const double di;//编译错误，const常量声明时未初始化 const double maxWage = 10.0; const double minWage = 0.5; const double *pc = &amp;maxWage; return 0; }","link":"/2014/07/19/Pointers-And-References/"},{"title":"Prepare C&#x2F;Cpp Development With VSCode On Mac&#x2F;Windows","text":"MacOS Catalina 打开App Store安装Xcode直至完成 安装命令行工具 xcode-select --install 安装VS Code brew cask install visual-studio-code VS Code配置 安装”C/C++ Extension”，该过程会自动安装其依赖的相关插件 打开一个工作空间，在界面输入”Command+Shift+P”，运行”C/C++ Edit Configurations (JSON)” 在打开的配置文件中的includePath中添加 &quot;/Library/Developer/CommandLineTools/usr/include/c++/v1&quot;, &quot;/usr/local/include&quot;, &quot;/Library/Developer/CommandLineTools/usr/lib/clang/12.0.0/include&quot;, &quot;/Library/Developer/CommandLineTools/usr/include&quot; 在打开的配置文件中的macFrameworkPath中添加 &quot;/System/Library/Frameworks&quot;, &quot;/Library/Frameworks&quot; 在界面输入”Command+Shift+P”，运行”Tasks: Configure Task”，编辑内容如下 { &quot;version&quot;: &quot;2.0.0&quot;, &quot;tasks&quot;: [ { &quot;label&quot;: &quot;C++: clang++ build active file&quot;, &quot;command&quot;: &quot;clang++&quot;, &quot;type&quot;: &quot;shell&quot;, &quot;args&quot;: [ &quot;-g&quot;, &quot;${file}&quot;, &quot;-std=c++11&quot;, &quot;-o&quot;, &quot;${fileDirname}/${fileBasenameNoExtension}&quot; ], &quot;presentation&quot;: { &quot;echo&quot;: true, &quot;reveal&quot;: &quot;always&quot;, &quot;focus&quot;: false, &quot;panel&quot;: &quot;shared&quot; } } ] } 在界面输入”Command+Shift+P”，运行”Debug: Open launch.json”，编辑内容如下 { &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ { &quot;name&quot;: &quot;C/C++ Launch active file&quot;, &quot;type&quot;: &quot;cppdbg&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;program&quot;: &quot;${fileDirname}/${fileBasenameNoExtension}&quot;, &quot;args&quot;: [], &quot;stopAtEntry&quot;: false, &quot;cwd&quot;: &quot;${workspaceFolder}&quot;, &quot;environment&quot;: [], &quot;externalConsole&quot;: false, &quot;MIMode&quot;: &quot;lldb&quot;, &quot;preLaunchTask&quot;: &quot;C++: clang++ build active file&quot; } ] } 新建cpp/hello.cpp文件，输入以下内容，看看智能代码提示是否生效 #include &lt;iostream&gt; using namespace std; int main() { cout&lt;&lt;&quot;Hello&quot;&lt;&lt;endl; } 测试运行，在运行窗口可看到刚配置的”C/C++ Launch”任务，运行将弹出Terminal显示Hello以及一些辅助信息 测试调试，在cpp/hello.cpp第5行添加断点，将看到断点调试效果 Windows 10在Windows 10上搭建VS Code C/C++开发环境（基于WSL服务） 按Win键搜索功能”Windows Features” 在弹出窗口中勾选”Windows Subsystem for Linux”项 打开Microsoft Store搜索Ubuntu并开始安装（也可凭个人爱好选用其他的发行版） 安装成功后进入启动初始化 初始化完成后，进入系统按需修改APT源# 备份配置文件 sudo cp -a /etc/apt/sources.list /etc/apt/sources.list.bak # 使用国内源（以华为镜像源为例） sudo sed -i &quot;s@http://.*archive.ubuntu.com@http://mirrors.huaweicloud.com@g&quot; /etc/apt/sources.list sudo sed -i &quot;s@http://.*security.ubuntu.com@http://mirrors.huaweicloud.com@g&quot; /etc/apt/sources.list 安装开发所需软件sudo apt update sudo apt install gcc gdb 从官网下载安装VSCode VS Code配置 安装”Remote WSL”插件，安装后会自动连接WSL并在WSL上下载安装VSCode Server 打开一个工作空间，安装”C/C++ Extension” 插件安装后，正常情况下会自动在空间下配置好tasks.json和launch.json 可进行运行和调试 至此，环境配置结束。","link":"/2020/12/27/Prepare-C-Cpp-Development-With-VSCode-On-Mac-Windows/"},{"title":"Pseudo Devices","text":"Device nodes on Unix-like systems do not necessarily have to correspond to physical devices. Nodes that lack this correspondence form the group of pseudo-devices. They provide various functions handled by the operating system. Some of the most commonly-used (character-based) pseudo-devices include: /dev/nullAccepts and discards all input; produces no output.（接受并丢弃所有输入；不产生输出） /dev/zeroProduces a continuous stream of NUL (zero value) bytes.（产生持续的NUL字节流） /dev/randomProduces a variable-length stream of pseudo-random or truly random numbers. (Blocking)（阻塞地生成变长伪随机或真随机数字流）(拓展阅读：/dev/random产生的随机数是否真随机？） /dev/urandomProduces a variable-length stream of pseudo-random numbers. (Non-Blocking)（非阻塞地生成变长伪随机数字流） /dev/fullProduces a continuous stream of NUL (zero value) bytes when read, and returns a “disk full” message when written to.（被读取时，生成时序的NUL字节流；被写入时，返回“磁盘已满”信息）","link":"/2015/08/19/Pseudo-Devices/"},{"title":"PyPI Module: ipgeo","text":"几天前在逛Github时发现huacnlee大神用Ruby写的Gem：ip-location。刚好我从未在PyPI上发布过模组，可以借此机会做一个Python语言的Port学习下如何发布到PyPi。 模组的介绍是这样的： ipgeo is a python module inspired by another ruby gem huacnlee/ip-location. The pypi page of ipgeo is https://pypi.python.org/pypi/ipgeo . ipgeo is used to retrieve geo info for ip address via taobao ip service(http://ip.taobao.com) . The Limitation of ipgeo is the query frequency should be less than 10 per second (http://ip.taobao.com/restrictions.php). For more information, please visit ipgeo's github page (https://github.com/caiski/ipgeo) . 首先说明此模组是由huacnlee的ip-location启发而来，内部实现逻辑也完全一样，只是语言不同。实现非常简单，有兴趣的同学可到这个Repository查看源码。 该模组是通过淘宝IP地址服务实现，因此要受淘宝IP地址服务的QPS限制。 要编写符合PyPI要求的模组工程，还需要写好setup.py这个文件，感觉就像是为这个模组编写Meta信息。 from distutils.core import setup setup( name = 'ipgeo', packages = ['ipgeo'], version = '0.2.3', description = 'Geo info retriver for ipv4 address using chinese taobao service', author = 'Ai Chao', author_email = 'aichaoguy@live.com', url = 'https://github.com/aichaoguy/ipgeo', download_url = 'https://github.com/aichaoguy/ipgeo/tarball/0.2.3', keywords = ['ip', 'geo', 'taobao'], classifiers = [ &quot;Programming Language :: Python&quot;, &quot;Programming Language :: Python :: 2.6&quot;, &quot;Programming Language :: Python :: 2.7&quot;, &quot;Programming Language :: Python :: 3&quot;, &quot;Environment :: Other Environment&quot;, &quot;Operating System :: OS Independent&quot;, &quot;Topic :: Software Development :: Libraries :: Python Modules&quot;, ], ) 各种本地工作完成后，便是打包和上传了。在.pypirc中设置号自己PyPI的账号密码后，使用Python自带的工具可以轻松完成。 python setup.py dist upload后续版本更新时，要对setup.py文件中version、download_url等选项的同步更新。 一个简单的意义甚微的PyPI模组就这样诞生了。","link":"/2014/12/26/PyPI-Module-ipgeo/"},{"title":"Reactor And Proactor","text":"在高性能I/O设计中，有两个比较著名的模式Reactor和Proactor模式，其中Reactor模式用于同步I/O，而Proactor运用于异步I/O操作。 先明白几个概念，同步 VS 异步， 阻塞 VS 非阻塞。 同步和异步是针对应用程序和内核的交互而言的。 同步：用户进程触发IO操作并等待或者轮询的去查看IO操作是否就绪。异步：用户进程触发IO操作以后，便开始做其他事情，而当IO操作已经完成的时候，用户进程会得到IO完成的通知。 阻塞和非阻塞是针对于进程在访问数据的时候，根据IO操作的就绪状态来采取的不同方式。说白了是一种读取或者写入操作函数的实现方式； 阻塞：读取或者写入函数将一直等待非阻塞：读取或写入函数会立即返回一个状态值 同步: 阻塞和非阻塞同步阻塞IO在此种方式下，用户进程在发起一个IO操作，必须等待IO操作的完成，只有当真正完成IO操作以后，用户进程才能继续运行。Java传统IO模型属于这一种。 同步非阻塞IO 为避免理解偏差，请同时参考此链接 在此种方式下，用户进程发起一个IO操作以后后边可返回做其它事情，但是用户进程需要时不时询问IO操作是否就绪，这就要求用户进程不停去询问，从而进入不必要的CPU资源浪费。其中目前Java的NIO就属于同步非阻塞IO。 异步: 阻塞和非阻塞异步阻塞IO此种方式下，指应用发起一个IO操作，不等待内核操作的完成，可以去执行其他内容。等内核完成IO操作以后会通知应用程序。 这其实就是同步和异步最关键的区别，同步必须等待或者主动地询问IO是否完成。底层是通过select系统调用来完成的，而select本身是阻塞的，采用select函数有个好处就是它可以同时监听多个文件描述符，从而提高系统并发性。 异步非阻塞IO此种模式下，用户进程只需要发起一个IO操作然后立即返回，等IO操作真正的完成以后，应用程序会得到IO操作完成的通知，此时用户进程只需要对数据进行处理就好了，不需要进行实际的IO读写操作，因为真正的IO读取或者写入操作已经由内核完成了。目前Java还没有支持此种模型。 Reactor 和 Proactor基于这些基本概念，在来看看Reactor模式和Proactor模式。 Reactor模式以读取操作为例， 具体步骤： 应用程序注册读就绪事件，和相关联的事件处理器 事件分离器等待事件的发生 当发生就绪事件时，事件分离器调用第1步注册的事件处理器 事件处理器首先执行实际的读取操作，然后根据读取到的内容进行进一步的处理 Proactor模式以读取操作为例， 具体步骤： 应用程序初始化一个异步读取操作，然后注册相应的事件处理器，此时事件处理器不关注读取就绪事件，而是关注读取完成事件，这是区别于Reactor的关键。 事件分离器等待读取操作完成事件 在事件分离器等待读取操作完成的时候，操作系统调用内核线程完成读取操作，并将读取的内容放入用户传递过来的缓存区中。这也是区别于Reactor的一点，Proactor中，应用程序需要传递缓存区。 事件分离器捕获到读取完成事件后，激活应用程序注册的事件处理器，事件处理器直接从缓存区读取数据，而不需要进行实际的读取操作。 Proactor的优势 Proactor模式消除了Fork和上下文切换的开销; Proactor的劣势与其他设计模式类似: 应用复杂度增加了. 与线性编程模型不同, 这里要将不同操作的回调函数写到不同的连接处理函数中. 异步事件难于调试. 因为所有操作是异步的, 难以追踪程序的执行顺序. 总结 同步和异步是相对于应用和内核的交互方式而言的，同步需要等待或者主动询问，而异步则是内核在IO事件发生的时候通知应用程序; 阻塞和非阻塞仅仅是系统在call系统调用时候的函数实现方式而已; 参考Reactor Sequence Diagram http://www.slideshare.net/ConversionMeetup/dejan-pekter-nordeus-reactor-design-pattern","link":"/2017/03/05/Reactor-And-Proactor/"},{"title":"Recommendation For Two Screenshot Software On Mac","text":"ScreenShot Plus国区已下线 2022.03.06国区售价￥12，支持全屏幕、定时捕捉、特定窗口、拖拽窗口。 App Store链接 Xnip - 支持滚屏的截图工具最大特色是支持滚屏，即长图截屏。 收费版可去除图片外框上的水印，订阅价：￥12/年。 App Store链接","link":"/2018/02/23/Recommendation-For-Two-Screenshot-Software-On-Mac/"},{"title":"Scrapy","text":"定义：Scrapy, An open source and collaborative framework for extracting the data you need from websites. In a fast, simple, yet extensible way.请注意上面的定义，即“什么是Scrapy”，或者说“Scrapy是什么”，这是最最重要的。 关于如何用Scrapy写爬虫，这里不做叙述，官网有详细的中英文文档。 Scrapy中Item和Pipeline的概念，将资源变为可自由定义、扩展的元素，很利于写出健壮、可维护的爬虫项目。 以下是我的一份爬取网络生效资源的Scrapy项目代码，思想比较简单，为了写出这个项目，从零开始接触Scrapy到完成内容爬取花了半个下午的时间。 在items.py中定义Itemclass SeItem(scrapy.Item): name = scrapy.Field() # 以下字段用于后期的文件下载 files = scrapy.Field() file_urls = scrapy.Field() 在pipelines.py中定义Pipelineclass SePipeline(FilesPipeline): def file_path(self, request, response=None, info=None): # start of deprecation warning block (can be removed in the future) def _warn(): from scrapy.exceptions import ScrapyDeprecationWarning import warnings warnings.warn('FilesPipeline.file_key(url) method is deprecated, please use ' 'file_path(request, response=None, info=None) instead', category=ScrapyDeprecationWarning, stacklevel=1) # check if called from file_key with url as first argument if not isinstance(request, Request): _warn() url = request else: url = request.url # detect if file_key() method has been overridden if not hasattr(self.file_key, '_base'): _warn() return self.file_key(url) # end of deprecation warning block media_guid = hashlib.sha1(to_bytes(url)).hexdigest() media_ext = os.path.splitext(url)[1] return 'full/%s%s' % (request.meta['name'][0].encode('utf8'), media_ext) # return 'full/%s%s' % (media_guid, media_ext) def get_media_requests(self, item, info): for file_url in item['file_urls']: yield Request(file_url.strip(), None, 'GET', None, None, None, {'name': item['name']}) def item_completed(self, results, item, info): if isinstance(item, dict) or self.files_result_field in item.fields: item[self.files_result_field] = [x for ok, x in results if ok] return item 比较麻烦的一点在于，下载后的文件需要我根据网页内容指定。 即：程序根据网页上下文推断某链接下载完毕后将要存储的文件名 由于Pipeline的file_path方法没法接收Item参数，所以我在Request参数的meta中加入了文件存储名，可以在上面的代码中看出。其他的逻辑就很简单了。 代码在Github上，不妨Review一下。","link":"/2017/01/02/Scrapy/"},{"title":"Serious Writing Benefits us","text":"Having been very busy working on graduation thesis since last month, I figured out that serious writing should be very important and of great priority in my work from now on. Why do I make such a decision? There are mainly three reasons. Serious writing helps me think clearly. If I want to convince others of my thoughts, I have to make them very clear to myself first, which helps training my mind. If my ideas were really inspiring and significant, they surely will take me a very long time to think out. Serious writing can do a lot of things in this period. Whenever we write down something, it’s archived and searchable. It can help us in our future work and if it’s public, people can be aware of our progress. These being said, serious writing is a time-consuming job, especially when it’s for those know nothing about our topic. But it will never be a waste of time since it benefits us in above three aspects. It’s a good habit and should be persisted.","link":"/2015/04/01/Serious-Writing-Benefits-us/"},{"title":"Some Confusion And Thoughts","text":"毕业工作以来的一段时间，一直是茫然状态。 程序员优点之一的抽象思维能力，并没有在此时被召唤出来（苦笑） 1、时间不够用了。 这是最大的感受。想做的有很多，工作、健身、吉他、理财……太多了。工作是占比最大的，每天12小时以上，也和工作效率有关系，但有种永远不会结束的感觉。任何稍微空闲的时候也总是会想到工作，这样下来，有时会觉得很累。 神经紧张，是从高三起养成的恶习。记得高二上学期上网吧被班主任抓到（苦笑），被教育一顿后就一直觉得只要放松便是浪费生命，现在看来是很可笑，当时却真的这么做了。整个高二下和高三，还有本科、硕士，就这么过来了。极少说话，每做一件事情都如临大敌，一旦失败就开始怀疑自身——真是一台极好的机器人啊，不这样又能怎样呢（苦笑） 2、感觉不到提升。 除了代码熟练度有一点提升外，没有发觉任何进步。工作，变成了真正的“搬砖”。思维没有得到较好锻炼，效率没有任何提升。可能对自己期望过高，较小的提升没有总结沉淀下来吧。 觉得待在了一个是非之地……不多说了，免留口实。 3、容易被人和事影响。 跟同事聊天、去商店买菜，有时候窝心得很。心理上的坚强也是修炼的重要一环啊···只好多看书了。看书再被列到计划中的话，时间还够用么。 4、不知道出路在哪里。 心里一直有一个宏大的五年计划，时刻想着念着，有时竟然会发怵。我能做到吗？我能坚持下来吗？重要的是，Do I deserve it ? I really don’t fucking know. 前路依然弥漫着一切不可知。 2015年11月15日 夜","link":"/2015/11/15/Some-Confusion-And-Thoughts/"},{"title":"TEAM - Keishichô Tokubetsu Sôsa Honbu","text":"只有男人戏的日剧，最精彩的只有正面冲突戏码吗？ 《TEAM-警视厅特别犯罪搜查本部》，2014年首播的日剧，依然是刑事剧，依然是纯男性演职员，在表演和剧本上继承了日剧一贯的夸张成分，男人正面冲突戏成为剧中亮点。当然，剧中不乏宫斗元素，比如检查官和警察部门的勾心斗角。但对于观众中居多的年轻群体，帅气的演员形象、剧中人的决断符合理想化的他们对未来自己的期待。日剧的热血，即为年轻群体而生，也因年轻观众的存在而常具生命力。","link":"/2016/12/24/TEAM-Keishicho-Tokubetsu-Sosa-Honbu/"},{"title":"Thanksgiving In World of Warcraft","text":"World of Warcraft, one of the most popular MMORPG on the planet, started celebrating Thanksgiving holiday anually from 2008. This year the holiday event in the game world will last for 8 days, from 21st Nov to 28th Nov. Players show great passion like always before and just like to every other virtual seasonal holiday event. Thanksgiving holiday event is actually all about fortune and sharing, like its couterpart in real life. It says, Even during hard times, the Pilgrim’s Bounty celebration emerges across the lands of Azeroth, bringing with it the opportunity to reflect and give thanks for the nourishment the land has provided. This season is the perfect occasion to brush up on your cooking skills while sharing in a veritable cornucopia of delicious offerings found on the communal dining tables located outside capital cities. There’s also no better time to go on a wild turkey hunt, or, should wild turkeys prove too elusive, transform friend and foe into tasty turkey targets. The event gives player unique rewards including toys, pets, recipies, and achivements. Maybe now it’s time for you to leave all behind and throw yourself into the game, enjoying another spirit of fortune, sharing and cooking. For more information, check out on the WOW official site.","link":"/2016/11/16/Thanksgiving-In-World-of-Warcraft/"},{"title":"The Brief Introduction Of JVM G1","text":"JVM G1垃圾回收算法简要介绍G1的特点 能够像CMS垃圾回收算法一样并发操作应用线程（潜台词：多核） 无需太长时间即可压缩空闲内存空间（潜台词：不会引起太多的GC停顿时间） 尽可能地让GC时长可控 不希望牺牲过多的吞吐量 不希望因此耗费大量更多的Heap空间 G1的优势G1作为CMS的长期替代品，有若干点优势： G1是一个压缩收集器，提供足够强的压缩来完全避免狭小的内存分配 依赖Regions概念，大大简化收集器逻辑，大部分情况下规避潜在的内存碎片问题 比CMS的GC停顿时长更加可预测，并允许用户指定停顿时长 G1的收集步骤Phase 1: Concurrent Global Marking. 并发扫描一遍之后，G1知道了哪些Region里大部分是空的（即大部分是可回收的对象），G1把收集和压缩操作集中于此，因此得名Garbage First. Phase 2: Evacuation. 并发地将一个或多个Region的资源拷贝至新的Region，压缩内存、拷贝、释放已拷贝完成的Region. 与此相比，CMS没有压缩内存（去除碎片）这一步，ParallelOld垃圾收集只进行全堆压缩. 细节：G1会估算Region的回收时间，以计算在用户指定时间内可回收多少（量力而行），因此G1不是『实时』收集器 ：） G1的建议使用场景拥有较大Heap空间（6GB及以上）、低停顿时长（0.5ms以内）要求的应用。 现在使用ParallelGC或CMS的应用，如果存在以下问题，可以考虑切换至G1： Full GC过于频繁、停顿时间过长的应用 对象分配比例经常发生剧变的应用 回收或压缩时间太长的应用（高于0.5s到1s） 如果使用ParallelGC或CMS没有发现以上现象，可以不必急于切换到G1，新版JVM仍然支持旧的垃圾回收算法。 G1的注意事项若从ParallelOldGC或CMS切换至G1，很可能会发现JVM运行内存变大了。这很大程度上要归因于 用于审计的两个数据结构『Remembered Sets』和『Collection Sets』 『Remembered Sets』每个Region都有一个，用于记录该Region中的对象引用，给G1提供并行独立收集各Region内存的可能性，它带来的负面影响不超5% 『Collection Sets』记录被收集的Region，它内部的所有数据会被收集干净，它带来的负面影响不超1%","link":"/2019/11/24/The-Brief-Introduction-Of-JVM-G1/"},{"title":"The SSH Protocol","text":"A few days ago, I was demanded to give a presentation about the SSH protocol, which I summarized as in below slide. Refer to this link SSH protocol has been one of the most fundamental infrastructures since 1990s, proving encrypted, secure remote control and other network services for millions of servers over unsecure network. It’s basically consisted of five different protocols, which are RFC 4250 The SSH Protocol Assigned Numbers, RFC 4251 The SSH Protocol Architecture, RFC 4252 The SSH Authentication Protocol, RFC 4253 The SSH Transportation Protocol and RFC 4254 The SSH Connection Protocol.","link":"/2015/09/29/The-SSH-Protocol/"},{"title":"The Sequence Of Multi-matches In Regex Expression","text":"正则表达式多选结构的顺序先看一道编程题 从一段只包含[.,0-9]字符的字符串中提取出全部可能的IPv4地址。 IPv4地址由十进制数和点来表示，每个地址包含4个十进制数，其范围为0-255,比如172.16.254.1；同时，4个十进制数不会以0开头，比如172.16.254.01是不合法的。 现输入一段文本str=&quot;1.1.1111.16..172.16.254.1.1&quot;，要求用程序答出该字符串所有子串可能形成的IPv4地址。 很多人会想到，IPv4的正则表达式我可熟悉啦，肯定能快速完成！于是从网上搜索到IPv4的正则表达式，写出了以下代码： // Java语言 import java.util.*; import java.util.regex.*; class Solution { private static final Pattern IPV4_PATTERN = Pattern.compile(&quot;((([1-9][0-9]?)|(1[0-9]{2})|(2[0-4]\\\\d)|(25[0-5])|0)\\\\.){3}&quot; + &quot;(([1-9][0-9]?)|(1[0-9]{2})|(2[0-4]\\\\d)|(25[0-5])|0)&quot;); public Set&lt;String&gt; findAllIpv4(String input) { Set&lt;String&gt; s = new TreeSet&lt;String&gt;(); Matcher m = IPV4_PATTERN.matcher(input); int from = 0; while (m.find(from)) { s.add(input.substring(m.start(), m.end())); from++; } return s; } } 我们看完这一段代码，可以确定的是：1. 正则表达式没问题，正确的IPV4地址可以用它来验证；2. Java Regex API用法也没有太大问题，基本符合预期。 输入：0.0.0.255 输出：[0.0.0.25] 但这段代码的结果是不正确的，正确输出是[0.0.0.2, 0.0.0.25, 0.0.0.255]，原因就出在正则中多选结构|的用法上。 多选结构（Alternation）多选结构在不同的正则引擎中，工作原理是截然不同的。在传统型NFA引擎中，会按照从左到右的顺序检查表达式中的多选分支，一旦可以匹配完成，其他的多选分支就不会尝试了。[^1] 以上节的正则表达式为例，我们单独摘出每个十进制数的表达式(([1-9][0-9]?)|(1[0-9]{2})|(2[0-4]\\\\d)|(25[0-5])|0)，它会以如下顺序进行匹配： 1. ([1-9][0-9]?) # 一位数或两位数 2. (1[0-9]{2}) # 位于区间[100-199]的三位数 3. (2[0-4]\\\\d) # 位于区间[200-249]的三位数 4. (25[0-5]) # 位于区间[250-255]的三位数 5. 0 # 0 那么在针对输入字符串0.0.0.255的匹配过程中，在进行第四个十进制数255的匹配时，会优先计算表达式([1-9][0-9]?)，这样可以匹配到25和2，而?(question mark)在正则中是一个贪婪量词[^2]，因此仅会留下25，所以最终我们看到了运行结果是[0.0.0.25]。 以这些知识为前提，我们可以通过优先匹配多位数字，手工解析少量数字的方式得到正确的解答程序，这样做的话，正则表达式需要做一些调整，将多位数匹配的多选分支放在前面，即(25[0-5]|2[0-4]\\\\d|1\\\\d{2}|[1-9]\\\\d|\\\\d)，代码如下： // Java语言 import java.util.*; import java.util.regex.*; class Solution { private static final Pattern IPV4_PATTERN = Pattern.compile(&quot;((25[0-5]|2[0-4]\\\\d|1\\\\d{2}|[1-9]\\\\d|\\\\d)\\\\.){3}&quot; + &quot;(25[0-5]|2[0-4]\\\\d|1\\\\d{2}|[1-9]\\\\d|\\\\d)&quot;); public Set&lt;String&gt; findAllIpv4(String input) { Set&lt;String&gt; s = new TreeSet&lt;String&gt;(); Matcher m = IPV4_PATTERN.matcher(input); int from = 0; int lastDotIdx = 0; String sub = null; while (m.find(from)) { sub = input.substring(m.start(), m.end()); s.add(sub); lastDotIdx = sub.lastIndexOf('.'); if (lastDotIdx == sub.length() - 3) { s.add(sub.substring(0, sub.length() - 1)); } else if (lastDotIdx == sub.length() - 4) { s.add(sub.substring(0, sub.length() - 1)); s.add(sub.substring(0, sub.length() - 2)); } from++; } return s; } } [^1]: Friedl, J. E. (2006). Mastering regular expressions. “ O’Reilly Media, Inc.”, p174-p175. [^2]: Friedl, J. E. (2006). Mastering regular expressions. “ O’Reilly Media, Inc.”, p142.","link":"/2019/12/21/The-Sequence-Of-Multi-matches-In-Regex-Expression/"},{"title":"Toy With ComboBox","text":"本文对ComboBox的各种函数进行了尝试。 #include &quot;StdAfx.h&quot; #include &lt;windows.h&gt; #include &lt;windowsx.h&gt; #include &quot;resource.h&quot; #include &quot;MainDlg.h&quot; BOOL WINAPI Main_Proc(HWND hWnd, UINT uMsg, WPARAM wParam, LPARAM lParam) { switch(uMsg) { HANDLE_MSG(hWnd, WM_INITDIALOG, Main_OnInitDialog); HANDLE_MSG(hWnd, WM_COMMAND, Main_OnCommand); HANDLE_MSG(hWnd,WM_CLOSE, Main_OnClose); } return FALSE; } BOOL Main_OnInitDialog(HWND hwnd, HWND hwndFocus, LPARAM lParam) { HWND hwnd_combo = GetDlgItem(hwnd,IDC_COMBO); ComboBox_InsertString(hwnd_combo,-1,TEXT(&quot;第三个&quot;)); ComboBox_InsertString(hwnd_combo,-1,TEXT(&quot;第二个&quot;)); ComboBox_InsertString(hwnd_combo,-1,TEXT(&quot;第一个&quot;)); ComboBox_InsertString(hwnd_combo,-1,TEXT(&quot;第零个&quot;)); return TRUE; } void Main_OnCommand(HWND hwnd, int id, HWND hwndCtl, UINT codeNotify) { HWND hwnd_combo = GetDlgItem(hwnd,IDC_COMBO); switch(id) { case IDC_BUTTONADD: { TCHAR strin[256]; GetDlgItemText(hwnd,IDC_EDITIN,strin,sizeof(strin));//取内容 ComboBox_InsertString(hwnd_combo,-1,strin);//添加至ComboBox SetDlgItemText(hwnd,IDC_EDITIN,TEXT(&quot;&quot;));//清空输入文本框 } break; case IDC_BUTTONDEL: { int i = ComboBox_GetCurSel(hwnd_combo);//取被选中的索引 if(CB_ERR == i)//如果没有选任何项，则报错 { MessageBox(hwnd,TEXT(&quot;你没有选中任何项&quot;),TEXT(&quot;错误&quot;),MB_OK|MB_ICONERROR); return; } ComboBox_DeleteString(hwnd_combo,i);//删除选中项 } break; case IDC_BUTTONFIND: { TCHAR strToSearch[256]; GetDlgItemText(hwnd,IDC_EDITOUT,strToSearch,sizeof(strToSearch));//得到要查找的内容 int iCount = ComboBox_GetCount(hwnd_combo); BOOL sFound = FALSE; int i; for(i=0;i&lt;iCount;i++)//遍历各项，若与要查找的内容相同，则弹出“找到了”，并选中该内容 { TCHAR str[256]; ComboBox_GetLBText(hwnd_combo,i,str); if(lstrcmp(str,strToSearch)==0) { sFound = TRUE; } if(sFound) { ComboBox_SetCurSel(hwnd_combo,i); MessageBox(hwnd,TEXT(&quot;找到了&quot;),TEXT(&quot;消息&quot;),MB_OK); break; } } if(FALSE == sFound) { MessageBox(hwnd,TEXT(&quot;没找到&quot;),TEXT(&quot;报告&quot;),MB_OK); return; } } break; default: break; } } void Main_OnClose(HWND hwnd) { EndDialog(hwnd, 0); } 实现三个基本功能： 点击“增加”按钮，实现把左侧文本框中字符串加入到ComboBox中作为一个项； 点击“删除所选”按钮，删掉ComboBox正被选择的那一项； 点击查找，将所选项的序号写入左侧文本框中。","link":"/2010/11/11/Toy-With-ComboBox/"},{"title":"Trial Of Bandwagon","text":"BandWagon是美国一家VPS服务商，一个月前在知乎上找到了推荐，我购买了其$2.99月付的10G Plan，相对于一年前的价格已有所上升。 首先说说网络速度。我选择的是Arizona机房，通过站长工具得到的结果如下： PING值维持在200ms以上，并不低。在使用Shadowsocks时，传输速度最快能达到1MB/s，但大多数情况只是几十KB的速度。配置虽然有限，256MB的内存应对Ubuntu 14 + Shadowsocks Server + Nginx Proxy还是能够应付得了的，但目前也不打算在它上面运行更多的东西，只当一个转发器使用。 值得一提的是，Bandwagon对CentOS系统提供了Open VPN和ShadowSocks的一键安装和GUI管理，非常方便。 不过，在这一个月中VPS曾宕机过一回，当时我发现Shadowsocks突然断开，就去开了Ticket，20分钟内问题得到了解决，客服的行动还算迅速的。 最后，这是我在BandWagon上的Referral Link，有需要的同学可通过这个连接购买，谢过：）","link":"/2015/06/11/Trial-Of-Bandwagon/"},{"title":"Usage With ListBox, CheckBox, Timer","text":"本文对ListBox,CheckBox,Timer三种控件进行了学习。 #include &quot;stdafx.h&quot; #include &lt;windows.h&gt; #include &lt;windowsx.h&gt; #include &quot;resource.h&quot; #include &quot;MainDlg.h&quot; /* 本节介绍了CheckBox、ListBox、Timer三种控件的使用 */ BOOL WINAPI Main_Proc(HWND hWnd, UINT uMsg, WPARAM wParam, LPARAM lParam) { switch(uMsg) { HANDLE_MSG(hWnd, WM_INITDIALOG, Main_OnInitDialog); HANDLE_MSG(hWnd, WM_COMMAND, Main_OnCommand); HANDLE_MSG(hWnd,WM_CLOSE, Main_OnClose); } return FALSE; } void CALLBACK MyTimerProc(HWND hwnd, UINT message, UINT iTimerID, DWORD dwTime) { SYSTEMTIME stLocal; GetLocalTime(&amp;stLocal); TCHAR strTime[256]; wsprintf(strTime,&quot;%i年%i月%i日 %i:%i:%i&quot;, stLocal.wYear,stLocal.wMonth,stLocal.wDay,stLocal.wHour,stLocal.wMinute,stLocal.wSecond); SetDlgItemText(hwnd,IDC_EDITTIME,strTime); } BOOL Main_OnInitDialog(HWND hwnd, HWND hwndFocus, LPARAM lParam) { SetTimer(hwnd,0,1000,MyTimerProc); return TRUE; } void Main_OnCommand(HWND hwnd, int id, HWND hwndCtl, UINT codeNotify) { HWND hwnd_check1 = GetDlgItem(hwnd,IDC_CHECK1); int checkStatus = Button_GetCheck(hwnd_check1); switch(id) { case IDC_OK: { //::SendMessage(HWND_BROADCAST,WM_SYSCOMMAND,SC_MONITORPOWER,2); //关闭显示器的API HWND hwnd_check1 = GetDlgItem(hwnd,IDC_CHECK1); int checkStatus = Button_GetCheck(hwnd_check1); if(BST_CHECKED == checkStatus) { MessageBox(hwnd,TEXT(&quot;CheckBox被选中&quot;),TEXT(&quot;提示&quot;),MB_OK); } if(BST_UNCHECKED == checkStatus) { MessageBox(hwnd,TEXT(&quot;CheckBox未被选中&quot;),TEXT(&quot;提示&quot;),MB_OK); } } break; case IDC_CHANGE: { HWND hwnd_check1 = GetDlgItem(hwnd,IDC_CHECK1); int checkStatus = Button_GetCheck(hwnd_check1); if(BST_CHECKED == checkStatus) { Button_SetCheck(hwnd_check1,BST_UNCHECKED); } if(BST_UNCHECKED == checkStatus) { Button_SetCheck(hwnd_check1,BST_CHECKED); } } break; case IDC_LISTBUTTON: { HWND hwnd_list1 = GetDlgItem(hwnd,IDC_LIST1); ListBox_AddString(hwnd_list1,TEXT(&quot;新的一项&quot;)); /*类似的有ListBox_InsertString(hwnd_list1,int index,TEXT(&quot;&quot;)) ListBox_DeleteString(hwnd_list1,int index) ListBox_GetCurSel(hwnd_list1) ListBox_SetCurSel(hwnd_list1,int index)//其中index设置为-1意为取消所有选择 ListBox_GetCount(hwnd_list1)等等函数*/ } default: break; } } void Main_OnClose(HWND hwnd) { EndDialog(hwnd, 0); }","link":"/2010/11/13/Usage-With-ListBox-CheckBox-Timer/"},{"title":"Why I Dump Books Like They Were Not Mine","text":"In this QingMing Jie, one of Chinese traditional holidays, I clean and bag my old books, plan to sell them for a minimal charge. I posted a thread which expressed my desire on the campus forum and more than half of my old books had been sold in 4 hours. It goes so smoothly because of the books’ high value and their attractive price. But why do I bother to do this ? Some of the books are actually useless to me. About three years ago, I bought a few books about Art, Drawing, Japanese, which since then had never been touched after I read several pages. I had many ambitious ideas back then, thinking about a lot of possibilities that might happen to get myself to a different level. Now I know that’s over-ambitious and learn to narrow my limited attention to what I really need to do. Internet and PDAs accelerate the disappearance of paper books. I bought a Kindle before last Spring Festival, and have stayed with it for over two months during which I read over 12 books, the amount that I barely achieved in any half year before. I would never want to lift those large, heavy, bacteria-ful books again. However, maybe you will say that paper books are more environment-protective. According to a research published by market research company Cleantech Group, kindles can be more environment-protective than traditional paper books. I’m not sure if this is advertorial but as far as I can tell, kindle has made my life easier and more effective. I’ll still keep some paper books for memorization but they will not be the main battle field any more.","link":"/2015/04/05/Why-I-Dump-Books-Like-They-Were-Not-Mine/"},{"title":"Grpc Tutorial Step By Step","text":"gRpc入门步骤详解1. 安装Go 1.16、Python 3brew install go@1.16 brew install python3 2. 环境变量和其他配置# pip mirror mkdir -p ~/.pip cat &gt;~/.pip/pip.conf &lt;&lt;EOF [global] index-url = https://mirrors.aliyun.com/pypi/simple/ [install] trusted-host=mirrors.aliyun.com EOF # go path export GOPATH=/Users/apple/go export PATH=$PATH:$GOPATH/bin 3. 安装依赖库# go grpc gen go get -u github.com/golang/protobuf/protoc-gen-go # python grpc gen pip3 install grpcio pip3 install protobuf pip3 install grpcio_tools 4. 准备工程目录结构# prepare project directory mkdir -p grpc-demo cd grpc-demo go mod init grpc-demo # prepare proto-buf directory mkdir -p pb # prepare server directory mkdir -p server #prepare client directory mkdir -p client #prepare service directory mkdir -p service 5. 定义.proto协议文件及生成Go代码# file pb/hello_grpc.proto syntax = &quot;proto3&quot;; package service; option go_package = &quot;.;service&quot;; service Greeter { rpc SayHello (HelloRequest) returns (HelloReply) {} } message HelloRequest { string name = 1; } message HelloReply { string message = 1; } 生成Go版本的协议代码 protoc -I pb/ pb/hello_grpc.proto --go_out=plugins=grpc:service -I 后面指定proto文件存放目录，和proto文件--go_out=plugins=grpc:后面指定生成go代码存放的目录检查文件service/hello_grpc.pb.go是否成功生成 6. 编写Go Server代码及运行Server# file server/server.go package main import ( &quot;context&quot; &quot;fmt&quot; &quot;hello_grpc/service&quot; &quot;net&quot; &quot;google.golang.org/grpc&quot; &quot;google.golang.org/grpc/reflection&quot; ) type server struct{} func (s *server) SayHello(ctx context.Context, in *service.HelloRequest) (*service.HelloReply, error) { return &amp;service.HelloReply{Message: &quot;hello &quot; + in.Name}, nil } type GreeterServer interface { SayHello(context.Context, *service.HelloRequest) (*service.HelloReply, error) } func main() { // listen local port lis, err := net.Listen(&quot;tcp&quot;, &quot;:5000&quot;) if err != nil { fmt.Printf(&quot;fail to listen: %s&quot;, err) return } // create grpc server s := grpc.NewServer() // register function service.RegisterGreeterServer(s, &amp;server{}) reflection.Register(s) err = s.Serve(lis) if err != nil { fmt.Printf(&quot;fail to start server: %s&quot;, err) return } } 运行Server，若发现报错则于项目根目录先执行go mod tidy准备好依赖。 go run server/server.go 7. 编写Go Client代码及测试Client# file client/client.go package main import ( &quot;context&quot; &quot;fmt&quot; &quot;hello_grpc/service&quot; &quot;google.golang.org/grpc&quot; ) func main() { // connect server conn, err := grpc.Dial(&quot;:5000&quot;, grpc.WithInsecure()) if err != nil { fmt.Printf(&quot;fail to connect server: %s&quot;, err) return } defer conn.Close() // create new client c := service.NewGreeterClient(conn) // call rpc function r, err := c.SayHello(context.Background(), &amp;service.HelloRequest{Name: &quot;grpc&quot;}) if err != nil { fmt.Printf(&quot;fail to call rpc function: %s&quot;, err) return } fmt.Printf(&quot;rpc call success: %s&quot;, r.Message) } 运行Client测试 % go run client/client.go rpc call success: hello grpc 8. 编写Python Client并测试Client第一步，生成Python语言协议文件hello_grpc_pb2.py和hello_grpc_pb2_grpc.py mkdir -p python python3 -m grpc_tools.protoc -I pb/ --python_out=python/ --grpc_python_out=python/ pb/hello_grpc.proto 第二步，编写Python Client # file python/client.py import logging import grpc import hello_grpc_pb2 import hello_grpc_pb2_grpc def run(): with grpc.insecure_channel('localhost:5000') as channel: stub = hello_grpc_pb2_grpc.GreeterStub(channel) response = stub.SayHello(hello_grpc_pb2.HelloRequest(name='grpc')) print(&quot;call success: {}!&quot;.format(response.message)) if __name__ == '__main__': logging.basicConfig() run() 第三步，运行Python Client % python3 python/client.py call success: hello grpc! 工程打包下载本站地址","link":"/2021/07/03/grpc-tutorial-step-by-step/"}],"tags":[{"name":"golang","slug":"golang","link":"/tags/golang/"},{"name":"photography","slug":"photography","link":"/tags/photography/"},{"name":"thoughts","slug":"thoughts","link":"/tags/thoughts/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"emerg","slug":"emerg","link":"/tags/emerg/"},{"name":"mac","slug":"mac","link":"/tags/mac/"},{"name":"alfred","slug":"alfred","link":"/tags/alfred/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"iptables","slug":"iptables","link":"/tags/iptables/"},{"name":"blockchain","slug":"blockchain","link":"/tags/blockchain/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"cpp","slug":"cpp","link":"/tags/cpp/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"postgresql","slug":"postgresql","link":"/tags/postgresql/"},{"name":"shell","slug":"shell","link":"/tags/shell/"},{"name":"tex","slug":"tex","link":"/tags/tex/"},{"name":"algorithm","slug":"algorithm","link":"/tags/algorithm/"},{"name":"rpc","slug":"rpc","link":"/tags/rpc/"},{"name":"design pattern","slug":"design-pattern","link":"/tags/design-pattern/"},{"name":"tricks","slug":"tricks","link":"/tags/tricks/"},{"name":"ssh","slug":"ssh","link":"/tags/ssh/"},{"name":"regex","slug":"regex","link":"/tags/regex/"},{"name":"trial","slug":"trial","link":"/tags/trial/"}],"categories":[{"name":"Other","slug":"Other","link":"/categories/Other/"},{"name":"IT","slug":"IT","link":"/categories/IT/"}]}